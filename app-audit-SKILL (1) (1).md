---
name: app-audit
description: >
  Perform an exhaustive professional audit of any frontend application â€” regardless of domain,
  stack, size, or complexity. Trigger for: "audit my app", "deep code review", "security review",
  "performance review", "UX review", "accessibility audit", "review before launch",
  "optimize my app", "improve the design", "make it more professional", "standardize my code",
  "check i18n", "review my AI integration", or when a user shares a source file for serious
  analysis. Also trigger for PWAs, React/Vue/Svelte/vanilla JS apps, calculators, dashboards,
  trackers, tools, games, medical/fintech/legal/e-commerce/SaaS/creative/AI-powered apps.
  Covers: domain correctness, security, privacy, performance, state management, UI/UX,
  visual design, design language, brand identity, commercial readiness, product aesthetics,
  visual differentiation, polish, standardization, accessibility, browser compat, code quality,
  data integrity, i18n, AI/LLM risks, and architecture.
  For app-specific context files (pre-filled Â§0), see references/.
---

# Professional App Audit â€” Universal Framework

---

## Â§TRIAGE â€” MANDATORY AUDIT ROUTING (execute BEFORE reading the rest of this skill)

**This gate fires whenever a user asks to "audit", "review", or "analyze" an app or file without specifying which audit type.**

Before loading any audit framework, reading any source code, or filling any context block â€” **stop and ask the user which audit they want.** Present the following options:

| Option | Skill | What It Covers |
|--------|-------|----------------|
| **Full App Audit** | `app-audit` | Code quality, security, performance, accessibility, UX, data integrity, architecture, domain correctness, i18n, AI/LLM risks, visual design (standard depth), and forward-looking scenarios |
| **Design & Aesthetic Audit** | `design-aesthetic-audit` | Deep visual analysis â€” style classification, color science, typography craft, motion vocabulary, surface & atmosphere, brand identity, competitive positioning, design character system, and source material research |
| **Both (Companion Mode)** | `app-audit` + `design-aesthetic-audit` | Full app audit with expert-depth design analysis replacing the standard Â§E/P6 visual sections. Longest and most thorough option. |

**Use the `ask_user_input` tool to present these three choices.** Do not proceed until the user selects one.

**After selection:**
- **Full App Audit** â†’ Continue reading this skill from Â§ORCHESTRATION onward. Do NOT load `design-aesthetic-audit`.
- **Design & Aesthetic Audit** â†’ Stop reading this skill. Load and follow `design-aesthetic-audit/SKILL.md` instead.
- **Both (Companion Mode)** â†’ Continue reading this skill. Load `design-aesthetic-audit/SKILL.md` as well. Follow the companion integration protocol in that skill's Â§COMPANION section.

**Skip this triage ONLY when:**
- The user explicitly names which audit they want (e.g., "run the design audit", "do a security review")
- The user has already selected in a prior turn of this conversation
- The user says "continue" or "next part" during an in-progress audit

---

## ORCHESTRATION â€” How This Skill Works

This skill is a **living audit instrument**, not a static checklist. It adapts to the app being audited â€” its domain, its stakes, its architecture, its aesthetic identity, and the developer's intentions.

The auditor simultaneously holds every specialist lens:

| Lens | What They See |
|------|--------------|
| **Senior engineer** | Every class of bug, race condition, and architectural smell |
| **Security researcher** | Every surface that assumes trusted input but receives hostile input |
| **Domain specialist** | Whether the rules the code must satisfy are actually satisfied |
| **Performance engineer** | Milliseconds, bytes, render frames, memory pressure |
| **Visual designer** | Rhythm, hierarchy, contrast, polish, craft, intentionality |
| **Product designer** | Whether the app's visual design serves the right goal for its context â€” commercial credibility for paid tools, cognitive safety for high-stakes tools, emotional warmth for sensitive contexts, subject fidelity for community tools, aesthetic output quality for creative tools |
| **Brand strategist** | Whether the app has a distinctive, coherent visual identity appropriate to its nature â€” competitive differentiation for commercial products, insider authenticity for community products, subject resonance for topic-specific tools |
| **UX designer** | Where users get confused, frustrated, or lost |
| **Accessibility specialist** | Who is excluded and exactly why |
| **Copywriter** | Whether the words in the interface are clear, consistent, and human |
| **QA lead** | The edge case the developer never considered |
| **Compliance officer** | What regulators, standards bodies, and lawyers would flag |
| **Refactoring expert** | How to improve the code without breaking anything |
| **Forward-looking architect** | What this codebase becomes under growth, new features, and time â€” not just what it is now |
| **Adversarial scenario tester** | What a bad actor, an unusual user, or an unlucky sequence of events finds that normal testing never does |

**All lenses operate simultaneously.** A wrong displayed number is not just a logic bug â€” it is simultaneously a UX trust failure, a data integrity gap, and potentially a security or compliance issue depending on stakes.

---

## Â§0. APP CONTEXT BLOCK

> **Fill this in before writing any findings.** It is the specification the audit verifies against.
> Extract what you can from the source code. Ask the user only for what cannot be extracted.
> Verify domain rules with the user â€” the code may be wrong, which is exactly why the audit exists.

```yaml
# â”€â”€â”€ CROSS-AUDIT CONTINUITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Complete this block only when this is NOT the first audit of this app.
# Its purpose: prevent silent contradiction between audit sessions.
# A finding that contradicts a previously confirmed domain rule is a CONFLICT, not a correction â€”
# and must be surfaced explicitly, not silently applied.
Prior Audit Reference:
  Version Audited:    # The version number from the previous audit's Â§0
  Session Date:       # Approximate date of the prior audit (helps identify which session)
  Confirmed Rules:    # List every domain rule confirmed [Â§0-CONFIRMED] in the prior session
    - # e.g. "BASE_RATE = 0.008 â€” confirmed by user, session 1"
    - # e.g. "MAX_SESSIONS = 5 â€” confirmed by user, session 1"
  Confirmed Findings: # Any findings from the prior audit confirmed as real bugs
    - # e.g. "F-007: rounding error in dose calculation â€” confirmed CRITICAL"
  Conflicts Flagged:  # Any place where this session's findings differ from the prior session's
    - # Format: "CONFLICT: [prior session claimed X] vs [this session finds Y] â€” needs user confirmation"

# â”€â”€â”€ IDENTITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
App Name:      # e.g. "InvoiceFlow" / "HealthTrack" / "PixelEditor"
Version:       # e.g. "v2.1.4" â€” is this a single source of truth or scattered across the codebase?
Domain:        # e.g. "Invoice creation for EU freelancers" / "Medication dosage calculator"
Audience:      # e.g. "Small business owners" / "Nurses" / "Casual gamers" / "Data analysts"
Stakes:        # LOW (hobby/entertainment) | MEDIUM (productivity, money-adjacent) |
               # HIGH (real financial transactions, legal records) |
               # CRITICAL (medical, safety-critical, legal compliance required)
               # Stakes is a severity multiplier â€” wrong data in a CRITICAL app is always CRITICAL.

# â”€â”€â”€ TECH STACK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Framework:     # e.g. "React 18 (CDN)" / "Vue 3 + Vite" / "Vanilla JS" / "Svelte"
Styling:       # e.g. "Tailwind CSS (CDN)" / "CSS Modules" / "Styled Components" / "Plain CSS"
State:         # e.g. "useReducer + localStorage" / "Zustand" / "Redux Toolkit"
Persistence:   # e.g. "localStorage only" / "IndexedDB" / "REST API + localStorage cache"
Workers:       # e.g. "Blob Web Worker + Blob SW" / "Workbox SW" / "None"
Visualization: # e.g. "Recharts" / "D3.js" / "Chart.js" / "None"
Build:         # e.g. "Zero build tools â€” CDN only" / "Vite 5" / "Webpack 5"
External APIs: # e.g. "None" / "Stripe" / "OpenWeather" / "Anthropic Claude API"
AI/LLM:        # e.g. "None" / "OpenAI GPT-4o via fetch" / "Claude claude-sonnet-4-6, streaming"

# â”€â”€â”€ PLATFORM & LOCALE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Target Platforms: # e.g. "Desktop-first" / "Mobile-first PWA" / "Both (responsive)"
                  # Determines: touch targets, viewport units, safe-area-inset, hover media query
Locale / i18n:    # e.g. "English only, US" / "EN + FR + DE, LTR" / "Multi-locale + RTL (AR, HE)"
                  # None = i18n ignored. Any locale = Â§N activated.
Performance Budget: # e.g. "None defined" / "LCP < 2.5s, TTI < 5s on 3G" / "Bundle < 200kb gz"

# â”€â”€â”€ ARCHITECTURE CONSTRAINTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Non-negotiable. Every recommendation must respect these â€” even if they are suboptimal.
# Suggestions to change constraints are welcome but must be clearly marked as architectural
# proposals, not standard findings.
Constraints:
  - # e.g. "Single-file: ALL code in App.jsx â€” no multi-file imports"
  - # e.g. "Zero build tools â€” no bundling, minification, tree-shaking"
  - # e.g. "CDN-only: React/libs loaded from CDN at runtime, no npm"
  - # e.g. "localStorage: sole persistence, 5MB limit, no server"

# â”€â”€â”€ DESIGN IDENTITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The app's intentional visual and interactive character.
# This protects the aesthetic from being "standardized" into something generic.
# Extract from the code, or ask the user.
Design Identity:
  Theme:         # e.g. "Dark-first with cyan/teal accent" / "Clean minimal light" / "Playful colorful"
  Personality:   # e.g. "Precise and informative" / "Warm and approachable" / "Sleek and premium"
  Signature:     # e.g. "Animated background particles, OLED pitch-black mode, glowing accents"
                 # These are PROTECTED â€” the audit improves them but never removes them.

  # â”€â”€ PRODUCT & COMMERCIAL VISUAL IDENTITY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Used by Â§E8, Â§E9, Â§F6, Â§L4. Inferred from code + domain if not provided.
  # NOTE: Commercial fields (Visual Reference, Monetization Tier, Conversion) are
  # only activated for paid/freemium/professional products. Fan/free/community tools
  # use the aesthetic-fidelity framing instead â€” see Â§I.4 Aesthetic Context Analysis.
  Visual Reference:      # 2â€“3 apps or sources whose visual quality/feel this should match
                         # Paid tools: "Linear, Vercel" / Fan tools: "the official companion for X" or "the visual language of X itself"
  Emotional Target:      # The feeling the app should produce at first glance
                         # e.g. "Fast + trustworthy + premium" / "Warm + delightful + approachable"
                         # e.g. "Feels like it belongs to this world" / "Made by someone who really knows this"
  Visual Differentiator: # What makes this app visually memorable
                         # Paid: distinctive vs competitors / Free/Community: authentic to its subject and audience
  Monetization Tier:     # "Free (no revenue intent)" / "Free (community tool)" / "Freemium SaaS"
                         # "Paid consumer ($5â€“$30/mo)" / "Professional B2B ($50â€“$500/mo)" / "Enterprise"
                         # Determines which Â§E8 framing activates. "Free" = aesthetic framing only.
  Distribution Channel:  # Determines first-impression quality requirements
                         # e.g. "Direct URL share" / "App Store listing" / "Reddit/Discord community share"
                         # / "Product Hunt" / "Enterprise sales demo" / "GitHub README"

# â”€â”€â”€ DOMAIN RULES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Every formula, constant, rate, threshold, and business rule the code MUST implement correctly.
# This is the specification. Wrong values here â†’ wrong findings.
Domain Rules:
  - # List each as: RULE_NAME = value / formula / description
  - # e.g. "TAX_RATE = 0.21 (21% VAT, exclusive)"
  - # e.g. "All monetary values stored as integer cents â€” never float"
  - # e.g. "MAX_RETRY_ATTEMPTS = 3, TIMEOUT_MS = 10000"
  - # e.g. "Server timezone UTC+8, no DST; America: UTC-5/UTC-4 (DST applies)"

# â”€â”€â”€ TEST VECTORS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Known input â†’ expected output pairs for the app's core calculations.
# Pre-supply these if you have them â€” Â§A2 requires â‰¥3 for probability/financial apps.
# The audit will verify these against actual code output. Wrong output = finding.
# Format: "Input: {values} â†’ Expected output: {result} â€” Source: {where this comes from}"
Test Vectors:
  - # e.g. "Input: principal=1000, rate=0.05, years=10 â†’ Expected: 1628.89 â€” Source: formula spec"
  - # e.g. "Input: weight=70kg, dose_factor=0.5 â†’ Expected: 35mg â€” Source: medical reference"
  - # e.g. "Input: items=[A,B,C], pity=89 â†’ Expected: guarantee triggers at 90 â€” Source: official rates"

# â”€â”€â”€ CRITICAL USER WORKFLOWS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The 5â€“10 most important end-to-end user journeys. The audit traces each one step-by-step.
Workflows:
  1: # "New user â†’ onboarding â†’ first core action â†’ result"
  2: # "Returning user â†’ load state â†’ update â†’ verify output updates"
  3: # "Import external data â†’ preview â†’ confirm â†’ verify correctness"
  4: # "Export â†’ fresh device â†’ import â†’ verify round-trip fidelity"
  5: # "Power user with maximum data â†’ performance still acceptable"
  # Add more as needed.

# â”€â”€â”€ KNOWN ISSUES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Known Issues:
  - # What the developer already suspects is broken

# â”€â”€â”€ AUDIT SCOPE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Audit Focus:   # e.g. "Full audit" / "Prioritize security and correctness" / "Design + polish only"
Out of Scope:  # e.g. "Backend code (not provided)" / "Third-party payment widget"

# â”€â”€â”€ GROWTH CONTEXT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Used by Â§O Projection Analysis. The audit reasons about the app's future, not just its present.
App Maturity:             # e.g. "Prototype/MVP" / "Active development" / "Stable/maintenance mode"
Expected Scale:           # e.g. "Single user forever" / "10â€“50 users" / "Scaling to thousands"
Likeliest Next Features:  # Top 3â€“5 features most likely to be added next
                          # e.g. "User accounts", "CSV export", "Collaboration", "Mobile app"
Planned Constraint Changes: # e.g. "Moving from localStorage to a backend in 6 months"
                             # e.g. "None planned" / "Adding authentication next sprint"
```

---

## I. ADAPTIVE CALIBRATION

Before writing any finding, classify the app along three axes. These classifications determine which dimensions get the deepest scrutiny and where severity multipliers apply.

### Â§I.1. Domain Classification â†’ Severity Amplification

| Domain | Amplify These Dimensions | Stakes Default |
|--------|--------------------------|----------------|
| Medical / Health | Â§A1 Logic, Â§A-Rules, Â§B3 Validation, Â§C6 Compliance | CRITICAL |
| Financial / Fintech / Billing | Â§A1 Logic, Â§J1 Financial Precision, Â§C1 Auth, Â§C6 Compliance | HIGHâ†’CRITICAL |
| Gambling-adjacent / Gacha | Â§A2 Probability, Â§L5 Ethical Design, Â§C6 Age/Compliance | MEDIUMâ†’HIGH |
| E-commerce / Payments | Â§C1 Auth, Â§C2 XSS, Â§J1 Financial, Â§C6 PCI/GDPR | HIGH |
| Web3 / Crypto / Wallet | Â§C1 Key/Seed handling, Â§C2 Injection, Â§A1 Tx math, Â§C6 Jurisdiction | HIGHâ†’CRITICAL |
| Social / Presence / Multi-user | Â§C5 Privacy, Â§J4 Real-time, Â§C6 GDPR/CCPA | MEDIUMâ†’HIGH |
| Productivity / SaaS | Â§B State, Â§D Performance, Â§H Flows, Â§K Ops | MEDIUM |
| Creative / Media / Tools | Â§E Design, Â§D Assets, Â§G Compatibility | LOWâ†’MEDIUM |
| Data / Analytics / Dashboards | Â§A1 Math, Â§B Data, Â§I Visualization | MEDIUMâ†’HIGH |
| Game / Companion / Fan Tool | Â§A Domain Data, Â§E Design, Â§C Attribution | LOWâ†’MEDIUM |
| AI / LLM-Powered | Â§K5 AI Integration, Â§C2 Prompt Injection, Â§C5 Privacy, Â§A1 Output Correctness | MEDIUMâ†’HIGH |

### Â§I.2. Architecture Classification â†’ Failure Modes

| Architecture | Primary Failure Modes to Hunt |
|--------------|-------------------------------|
| Single-file monolith (CDN React) | Dead code accumulation, blob Worker browser incompatibility, no code-splitting, CSS specificity at scale |
| Multi-file SPA (Vite/Webpack) | Bundle bloat, stale chunks, tree-shaking failures, import cycles |
| SSR / Next.js | Hydration mismatches, server/client state divergence, SEO gaps |
| Vanilla JS | Global scope pollution, event listener leaks, DOM coupling |
| PWA (any) | SW versioning, cache poisoning, offline-first edge cases |
| LocalStorage-only | Quota exhaustion, schema migration gaps, concurrent-tab conflicts |
| Backend-connected | Race conditions, optimistic update failures, token leaks, CORS |

### Â§I.3. App Size â†’ Audit Scope

| Lines of Code | Parts | Depth |
|---------------|-------|-------|
| < 500 | 4â€“5 | All dimensions, condensed findings |
| 500â€“2,000 | 6â€“8 | Full dimensions, moderate findings |
| 2,000â€“6,000 | 8â€“12 | Full dimensions, detailed findings |
| 6,000â€“15,000 | 12â€“16 | Full dimensions + domain deep dives |
| > 15,000 | 16+ | Full dimensions + per-module sub-audits |

### Â§I.4. Aesthetic Context Analysis â€” Five Independent Axes

> **The core principle**: aesthetic goals are not derived from a single product category label. They are derived from five independent dimensions that can combine in any configuration. A meditation app, a nurse's dosing calculator, a generative art toy, a local community board, and a B2B dashboard all require entirely different aesthetic reasoning â€” even if some share the same business model or audience size.

> **Do not skip this analysis.** The five axes take two minutes to complete and prevent hours of wrong recommendations downstream. Every finding in Â§E, Â§F, and Â§L is shaped by the profile produced here.

---

#### AXIS 1 â€” Commercial Intent
*What role does trust-building and conversion play in this app's visual goals?*

| Level | Signal | Aesthetic implication |
|-------|--------|-----------------------|
| **Revenue-generating** | Paid tier, subscription, in-app purchase, ad-supported | Visual trust signals actively matter â€” every design choice either supports or undermines willingness to pay or engage financially |
| **Institutional / grant-funded** | Non-profit, government, educational, healthcare org | Credibility and legitimacy signals matter â€” visual design must communicate seriousness and compliance, not commercial appeal |
| **Non-revenue / freely given** | Free tool, open-source, community gift | Commercial signals are irrelevant or actively harmful â€” visual goals shift entirely to craft, clarity, and authenticity |

---

#### AXIS 2 â€” Use Intensity & Emotional Context
*What is the user's cognitive and emotional state when using this app? This determines how much the design can demand of their attention.*

| Mode | Examples | Aesthetic implication |
|------|----------|-----------------------|
| **Focus-critical / high-frequency** | Daily work tool, professional instrument, developer IDE companion | Design must be nearly invisible â€” efficiency, scanability, and zero-distraction above all. Every animation is a tax. |
| **High-stakes / low-frequency** | Medical dosing calculator, legal document tool, emergency reference | Cognitive load reduction is the primary aesthetic goal. Visual noise = danger. Calm, high-contrast, unambiguous. |
| **Emotionally sensitive** | Mental health tool, grief support, therapy companion, crisis resource | Safety, warmth, and calm are structural requirements â€” not stylistic choices. Harsh colors, abrupt transitions, or playful copy cause harm. |
| **Creative / exploratory** | Art generator, music composition tool, design sandbox, writing tool | The aesthetic can be expressive and surprising â€” discovery and inspiration are valid goals. Delight is functional, not decorative. |
| **Learning / progressive** | Educational app, tutorial tool, skill trainer | Visual design must communicate progress, reward effort, and reduce intimidation. Pacing and encouragement are aesthetic requirements. |
| **Leisure / casual** | Entertainment tracker, hobby companion, quiz app, idle tool | Delight is primary. Friction that would be tolerable in a work tool is unacceptable here. Polish and playfulness are both appropriate. |
| **Occasional / transactional** | Unit converter, flight tracker, calculator, lookup tool | Get in, get the answer, get out. Visual complexity above "clean and immediate" is waste. |

---

#### AXIS 3 â€” Audience Relationship to the Subject
*How much does the audience already know? This determines vocabulary, visual complexity, information density, and what "belonging" looks like.*

| Relationship | Examples | Aesthetic implication |
|--------------|----------|-----------------------|
| **Domain expert / practitioner** | Clinicians, engineers, developers, financial analysts | Information density is a feature. Precision vocabulary is required â€” generic language signals the tool doesn't understand the domain. Visual complexity can be high if it serves the data. |
| **Enthusiast / community member** | Hobbyists, fans, dedicated amateurs | Community vocabulary and aesthetic norms signal insider status. Generic design signals the maker is an outsider. Not knowing the community's natural language is a credibility failure. |
| **Casual / general public** | Anyone unfamiliar with the domain | Progressive disclosure is mandatory. Domain jargon must either be avoided or clearly explained. Intimidation is a design failure. |
| **Mixed / bridging** | A tool that serves both experts and beginners | Progressive disclosure architecture is critical. The design must not condescend to experts or overwhelm novices simultaneously. This is one of the hardest design problems â€” acknowledge it explicitly. |

---

#### AXIS 4 â€” Subject Visual Identity
*Does the subject the app is built around have an established visual language? This determines whether visual fidelity is a goal.*

| Identity strength | Examples | Aesthetic implication |
|-------------------|----------|-----------------------|
| **Strong established aesthetic** | A specific game, show, sport league, musical genre, cultural movement | The app has an opportunity â€” and arguably an obligation â€” to honor the visual language of its subject. Palette, typography weight, motion character, and copy tone should feel *inspired by* the source, not arbitrarily chosen. |
| **Community-defined aesthetic norms** | Hacker/developer culture, speedrunning community, tabletop RPG players, street photography | The community has visual conventions that signal insider status. Violating them (accidentally going corporate, or too polished, or wrong era) reads as not understanding the culture. |
| **Domain-defined visual conventions** | Medical interfaces, financial terminals, academic tools, engineering dashboards | Professional domains have established visual conventions that communicate seriousness and domain-appropriateness. Departing from them without reason creates distrust. |
| **Neutral / no established identity** | Abstract SaaS, general productivity tool, new utility | Visual language must be invented rather than inherited. This is both more freedom and more responsibility â€” there's no reference to honor, so coherence must come entirely from within. |
| **The aesthetic IS the subject** | Generative art tool, music visualizer, color palette builder, typography explorer | The product's visual output is its primary value. The UI chrome must recede â€” the subject is the visual experience, not the container. Overdesigning the interface steals attention from what the tool produces. |

---

#### AXIS 5 â€” Aesthetic Role in the Product's Value
*Is design serving function, communicating identity, or is it the product itself?*

| Role | Examples | Aesthetic implication |
|------|----------|-----------------------|
| **Aesthetic IS the value** | Generative art, music player, creative tool, animation playground | The interface aesthetic is inseparable from the product. Compromise here is product failure. |
| **Aesthetic amplifies the value** | Well-designed productivity app, thoughtfully crafted community tool | Good aesthetic makes a working product better â€” more trusted, more enjoyable, more recommended. Standard design investment applies. |
| **Aesthetic communicates identity** | Portfolio, cultural tool, brand-representing product, community-facing gift | The design is a statement about the maker and their relationship to the subject or audience. Authenticity matters as much as polish. |
| **Aesthetic must stay invisible** | Emergency tool, high-stress professional instrument, accessibility-first interface, crisis resource | Design that draws attention to itself is actively harmful here. The goal is zero interference with the user's cognitive task. Every "nice" design touch must be audited against: does this distract? |

---

#### APPLYING THE FIVE AXES

**Step 1**: Classify each axis from Â§0 signals, code, and domain. Record the classification explicitly at the start of the audit.

**Step 2**: Identify any axis conflicts. Some combinations create genuine tension that must be resolved deliberately:
- *High commercial intent + emotional sensitivity* (wellness app with subscription) â†’ commercial signals must be handled with exceptional care â€” trust comes from emotional safety first, conversion second
- *Expert audience + strong subject identity* (developer tool with hacker culture norms) â†’ density and terseness ARE the aesthetic â€” over-polishing violates community norms
- *Aesthetic IS the subject + high-stakes use* (medical imaging tool with visualization) â†’ the visualization must be designed with craft; the UI chrome around it must disappear
- *Mixed audience + domain aesthetic conventions* (medical app for patients AND clinicians) â†’ requires two visual registers in one product â€” acknowledge this as an architectural design challenge

**Step 3**: Derive the **Aesthetic Goal Profile** â€” a one-sentence summary for this specific app:
> *"The aesthetic goal for [app] is: [primary goal derived from axes], which means [2â€“3 specific implications for findings]."*

Example profiles:
- A free community tool for enthusiasts of a visually distinctive subject, used leisurely: *"Craft and subject fidelity â€” the app should feel made by someone who genuinely knows this world, not a polished product trying to monetize a community."*
- A high-frequency professional tool for domain experts, used in focus-critical contexts: *"Invisible efficiency â€” every design element must serve information retrieval speed. Delight is a distraction. Density is a feature."*
- A paid creative tool for mixed audiences: *"Expressive trust â€” the aesthetic must feel inspiring enough to justify the cost, accessible enough for newcomers, and restrained enough not to compete with what the user creates."*
- A free wellness app used in emotionally sensitive contexts: *"Calm safety â€” warmth and visual quiet are functional requirements. Nothing jarring, nothing demanding, nothing that competes with the user's emotional state."*

**Step 4**: Use the Aesthetic Goal Profile as the filter for every finding in Â§E, Â§F, and Â§L. A finding that contradicts the profile is wrong regardless of how valid it would be for a different app.

---

#### THE FIVE-AXIS LENS REFERENCE

Throughout Â§E, Â§F, and Â§L, findings are marked with the axis they primarily serve:

- `[A1]` Commercial intent axis â€” applies when revenue or institutional credibility is a goal
- `[A2]` Use context axis â€” applies when emotional state or cognitive load shapes the requirement  
- `[A3]` Audience axis â€” applies when expertise level shapes vocabulary or density
- `[A4]` Subject identity axis â€” applies when the subject has visual conventions to honor
- `[A5]` Aesthetic role axis â€” applies when aesthetic investment level or restraint is the question

When a finding is tagged `[A1]` â€” **skip or substantially reframe it if the app is non-revenue.** When a finding is tagged `[A4]` â€” **skip or substantially reframe it if the subject has no established visual identity.** And so on. The tags make the conditionality explicit rather than requiring the auditor to remember it.

---

### Â§I.5. Domain Rule Extraction

When the user doesn't provide domain rules, extract them from the code â€” but apply strict source discipline. Every extracted value is `[CODE]` until the user confirms it, at which point it becomes `[Â§0-CONFIRMED]`. Never assume a code value is correct just because it is present.

```javascript
// Named constants â†’ immediate Â§0 candidates
const TAX_RATE = 0.21           // â†’ [CODE: line N] â€” verify with user â†’ becomes [Â§0-CONFIRMED] only after user confirms
const MAX_ITEMS = 50            // â†’ [CODE: line N] â€” is this a spec value or an implementation guess?
const MAX_SESSIONS = 5          // â†’ [CODE: line N] â€” verify against authoritative spec

// Hardcoded numbers in formulas â†’ red flags requiring verification
if (score > 100) { ... }                   // â†’ [CODE: line N] â€” why 100? Spec rule or implementation assumption?
const rate = 0.08 + (i - 50) * ramp       // â†’ [CODE: line N] â€” why 0.08? Why 50? Need source. DO NOT assume from memory.
const total = items * 1.21                 // â†’ [CODE: line N] â€” hardcoded tax rate? Which jurisdiction? DO NOT recall from training.
const dose = weight * 0.5                  // â†’ [CODE: line N] â€” CRITICAL: DO NOT guess the correct coefficient. Ask.
```

Present extracted rules as:
> âœ“ `MAX_SESSIONS = 5` [CODE: line 342] â€” Â§0 confirms `[Â§0-CONFIRMED]`. Verified.
> âš  `ramp_divisor = 15` [CODE: line 989] â€” not in Â§0. Flagging for user confirmation before any findings use this value.
> ðŸš¨ `dose = weight * 0.5` [CODE: line 1204] â€” coefficient in code but unconfirmed. CRITICAL until confirmed. Not recalling a "correct" value from training â€” asking user to confirm the intended coefficient and its source reference.
> ðŸ”² `BASE_RATE` â€” needed for Â§A1 assessment but absent from code and Â§0. Cannot assess correctness. Deferring finding and flagging as audit gap.

**Domain-specific escalation triggers** â€” when these patterns appear in code, escalate automatically:

| Code Pattern | Auto-Escalation |
|-------------|-----------------|
| `dose`, `dosage`, `medication`, `mg`, `mcg` | All Â§A1 findings â†’ CRITICAL minimum |
| `payment`, `charge`, `billing`, `stripe` | All Â§C1, Â§C2 findings â†’ CRITICAL minimum |
| `balance`, `transaction`, `transfer` | All Â§A1, Â§B3 findings â†’ CRITICAL minimum |
| `float` used for monetary values | Automatic CRITICAL finding â€” money is never float |
| `age`, `minor`, `children` | All Â§C6 compliance findings â†’ HIGH minimum |
| `password`, `token`, `secret` in localStorage | Automatic CRITICAL finding |
| Probability displayed without worst-case | Â§A2, Â§L5 â€” HIGH if real money involved |

---

### Â§I.6. Knowledge & Source Integrity

Domain data fabrication is the most damaging error class in app audits â€” more harmful than a missed bug, because a fabricated fact produces false findings that waste developer time and erode trust in every other result. This section defines how every domain fact must be sourced, evaluated, and cited.

#### Source Classification

Every domain fact must carry one of four source tags:

| Tier | Tag | Meaning | Action |
|------|-----|---------|--------|
| Code-verified | `[CODE: line N]` | Value read directly from the provided source file | Ground truth for code *behavior* â€” still needs verification against Â§0 to confirm it is *correct* |
| User-confirmed | `[Â§0-CONFIRMED]` | Value provided or confirmed by the user in Â§0 | The specification â€” code is tested against this |
| Web-sourced | `[WEB: source, version, date]` | Value found via live web search | Quality tier determines whether it supports a finding â€” see Web Source Quality below |
| Training-recalled | `[UNVERIFIED]` | Value recalled from training data only | Never use as a finding basis â€” present as a question to the user |

**Source hierarchy for correctness claims** (strongest â†’ weakest):
`[Â§0-CONFIRMED]` â†’ `[WEB: official-docs]` â†’ `[WEB: patch-notes]` â†’ `[WEB: official-wiki]` â†’ `[CODE]` alone â†’ `[WEB: community-wiki]` â†’ `[WEB: forum]` â†’ `[UNVERIFIED]`

Only `[Â§0-CONFIRMED]` and `[WEB: official-docs/patch-notes]` are strong enough to assert "the code is wrong." Everything below supports a question or a flag â€” not a finding.

#### Web Source Quality

Web search has its own failure modes: outdated pages, version-mismatched docs, community wikis with unverified edits, and multiple sources contradicting each other on the same value.

| Quality | Tag | Examples | Supports correctness finding? |
|---------|-----|----------|-------------------------------|
| Official documentation | `[WEB: official]` | Developer's own API docs, official site, medical publisher, standards body | Yes â€” after version check |
| Official patch / release notes | `[WEB: patch-notes, vX.Y]` | Dated changelogs from the developer | Yes â€” if version matches the app under audit |
| Developer-maintained wiki | `[WEB: official-wiki]` | Wiki explicitly maintained by the studio or org | Yes â€” with date check |
| Community wiki | `[WEB: community-wiki, date]` | Player-maintained wikis, fan wikis | Conditional â€” only if recently verified and internally sourced |
| Forum / community post | `[WEB: forum, date]` | Reddit, Discord, community guides | No â€” lead only; requires corroboration from a higher-quality source |
| Aggregator / secondary | `[WEB: aggregator]` | Sites summarizing other sources | No â€” locate and cite the original source directly |

**Seven rules for web-sourced domain facts:**

1. **Prefer official sources always.** Do not cite a wiki when official docs exist.
2. **Always record version and date.** Tag as `[WEB: official-docs, v1.8, 2024-03]`. A correct value from last year may be wrong today.
3. **When sources disagree â€” surface the conflict, never pick a winner silently.**
   > "âš  SOURCE CONFLICT: Official API docs [WEB: official, 2025-01] state `MAX_TOKENS = 4096`. Community wiki [WEB: community-wiki, 2024-06] states `8192`. Please confirm which applies to your integration."
4. **When community wiki contradicts official docs â€” defer to official docs and flag the discrepancy.**
5. **When only community/forum sources exist â€” do not assert correctness. Ask.**
   > "ðŸ”² `BASE_RATE = 0.006` [CODE: line 412]. Only a community guide found â€” no official reference. Flagged as audit gap."
6. **Never silently prefer a web value over the code value.** A discrepancy must be surfaced, not auto-corrected.
7. **Never use training memory as a tiebreaker when web sources conflict.** The user arbitrates â€” not the auditor.

**Domain categories where web sources are especially unreliable:**
- Game mechanics and live-service constants: frequently updated per-patch; wiki edits lag announcements; community speculation presented as fact is endemic
- Third-party API limits: frequently tier-dependent and silently changed; documented limit â‰  enforced limit
- Medical reference ranges: must cite peer-reviewed publication or official clinical guideline â€” no wikis, no forums
- Community-derived formulas (datamining, reverse-engineering): approximations, not specifications â€” tag `[WEB: community-derived]` and flag uncertainty explicitly

**Domain categories where training data is especially unreliable â€” always treat as `[UNVERIFIED]` unless sourced:**
- Game mechanics, live-service constants, per-patch values (rates, thresholds, drop tables, cooldowns)
- Third-party API rate limits, pricing tiers, model context windows, service SLAs
- Medical reference ranges, drug interaction rules, clinical formula coefficients
- Financial regulation specifics (tax rates, reporting thresholds, jurisdiction rules)

#### Scenario Reference

| Situation | Correct output |
|-----------|----------------|
| Code matches Â§0 or official web source | âœ“ `RATE = 0.006` [CODE: line 412] matches [Â§0-CONFIRMED] / [WEB: official, v1.8, 2024-11]. Verified. |
| Code value, no Â§0, official web source found | âš  Official docs confirm this value. Flagging for user confirmation that this is the correct reference version. |
| Code value, web sources conflict | âš  SOURCE CONFLICT: Official docs: X. Community wiki: Y. Cannot assert correctness â€” please confirm which source applies. |
| Code value, only community sources | ðŸ”² Only community sources found. Flagged as audit gap; cannot confirm correctness. |
| Value needed, no source anywhere | ðŸ”² Value needed for Â§A1 assessment. No Â§0, no reliable web source found. Deferring correctness finding â€” audit gap. |

#### Cross-Session Consistency

When a second or subsequent audit occurs on the same app or a different version:
1. Surface the Prior Audit Continuity block from Â§0 at the top of Part 1
2. Prior `[Â§0-CONFIRMED]` rules carry forward as confirmed until explicitly contradicted by new Â§0 input
3. Any conflict between this session's findings and a prior confirmed rule surfaces immediately as `[CONFLICT]` â€” never silently resolved in either direction
4. Do not re-derive previously confirmed domain rules from training data â€” re-derive only from code + new user confirmation
5. Version differences are real: a constant correct in v1.2 may have changed in v1.3. Always check whether the app version has changed and flag which confirmed rules may be version-specific

---

### Â§I.7. Adaptive Analysis Protocols

#### Mid-Audit Reclassification Triggers
During the audit, if any of the following are discovered, **STOP and reclassify before continuing**. The initial Â§0 classification was based on incomplete information; these discoveries change the audit's severity baseline:

| Discovery | Reclassification Action |
|-----------|------------------------|
| Undisclosed financial transaction code | Escalate Stakes â†’ HIGH; activate Â§K1, Â§C1 |
| Undisclosed health / dosage calculations | Escalate Stakes â†’ CRITICAL immediately |
| State that persists PII to localStorage | Activate full Â§C5, Â§C6 GDPR review |
| CDN scripts without SRI in a payment/auth context | Immediate CRITICAL â€” supply chain attack surface |
| Code quality varies dramatically by section | Flag multiple-author / rush-commit sections; elevate confidence threshold for those sections |
| Dead code > 20% of total codebase | Adjust P10 scope â€” dead code analysis becomes primary; active/dead boundary must be mapped before other dimensions |
| Hardcoded credentials found | Surface as CRITICAL immediately; do not proceed without developer acknowledgment |
| Imports or calls to modules not present in provided files | Note as audit gap â€” affected findings are [THEORETICAL] until missing code is provided |

#### Partial Codebase Protocol
When only part of the codebase is provided:
1. Explicitly list what is **not** provided â€” `backend/`, auth module, worker file, etc.
2. Flag findings that depend on missing code as `[THEORETICAL]` â€” cannot confirm without full context
3. Do not assume missing code is correct â€” record its absence as a named audit gap
4. State which dimensions are affected by each gap and what would be needed to close it
5. Ask the user to provide the missing files before proceeding with deeply affected dimensions

#### Novel Pattern Protocol
When encountering a pattern not covered by the audit taxonomy:
1. Describe the pattern precisely â€” what it does, what it appears to intend, what it resembles
2. Classify by analogy: "behaves like a state machine implemented via X instead of Y"
3. Apply the nearest applicable dimension's criteria and note the approximation explicitly
4. Flag: `[NON-STANDARD PATTERN â€” audit criteria approximated via Â§X analogy]`
5. Note what cannot be assessed without understanding the original intent

#### Code Quality Variance Signal
If code quality varies significantly between sections â€” professional in some areas, rushed or inconsistent in others:
- This is a strong signal of **multiple authors**, **time-pressure commits**, or **copy-paste from external sources**
- Identify which sections have lower quality and apply elevated scrutiny to those sections
- Critical risk flag: if lower-quality sections are also the ones handling higher-stakes logic (payments, medical, auth), this is the **highest-risk combination** in the codebase â€” escalate all findings in those sections by one severity level

#### Signal Correlation â€” Connecting Distant Patterns
Some bugs are only visible when two distant code locations are read together:
- A constant defined early, used incorrectly hundreds of lines later
- A validation rule in one component that contradicts business logic in another
- State initialized correctly but reset incorrectly in a cleanup function far away
- A security assumption in one layer silently violated by a different layer

**Protocol**: For every validation rule, find every place that validates the same concept â€” do they all agree? For every security assumption, trace whether anything downstream silently violates it. Cross-reference findings across sections before finalizing severity.

---

## II. AUDIT PHILOSOPHY â€” THE IRON LAWS

### Law 1 â€” Specificity Is Non-Negotiable
Every finding names the exact function, variable, line number (or `near functionName`), CSS class, or data value. "Improve error handling" is not a finding. "`handleImport()` near line 847 calls `JSON.parse()` without a try/catch â€” any non-JSON clipboard paste throws an uncaught TypeError that crashes the React tree" is a finding.

### Law 2 â€” Bugs Before Refactors, Always
If a function has a bug AND is poorly structured: fix the bug first, with a minimal targeted change. The structural improvement is a separate, lower-priority recommendation. Never bundle them â€” it multiplies regression risk and obscures what actually changed.

### Law 3 â€” Honesty Over Completeness
`[THEORETICAL]` with clear reasoning is infinitely more valuable than `[CONFIRMED]` that is fabricated. Never invent line numbers, function names, or behavior. When uncertain, say so explicitly and explain why you suspect the issue.

Every domain fact must carry a source tag. Any fact tagged `[UNVERIFIED]` is a question to the user, not a finding. The full source classification system, web source quality tiers, and scenario reference are in **Â§I.6**.

### Law 4 â€” The Feature Preservation Contract
Every working feature is innocent until proven broken. The Feature Preservation Ledger (built in Part 1) is a binding contract: **no recommendation may break, remove, or diminish a working feature.** This applies to optimization, polishing, standardization, and refactoring recommendations equally. If a simplification would remove a working feature â€” it is rejected, full stop.

### Law 5 â€” The Identity Preservation Contract
The app's intentional design character (from Â§0 Design Identity) must not be erased by the audit. Polishing means making the existing vision more refined and consistent â€” not replacing it with generic conventions. A dark cyberpunk aesthetic with glowing accents gets polished *as* a dark cyberpunk aesthetic, not converted into a neutral gray corporate dashboard.

### Law 6 â€” Stakes Multiply Severity
Wrong numbers in a hobby tracker â†’ MEDIUM. The same wrong numbers in a medical dosing app â†’ CRITICAL. Use the Stakes field from Â§0 as a severity multiplier across all categories.

### Law 7 â€” The Golden Question
Before every modification recommendation:
> *"If the developer applies this change at 2 AM without carefully reviewing it, what is the worst realistic outcome?"*
> If the answer is anything other than "nothing bad" â€” add explicit warnings, reduce scope, or split into safer atomic steps.

### Law 8 â€” Minimum Footprint
Every fix recommendation must use the smallest safe change that resolves the problem. A 3-line targeted fix is strictly preferred over a refactor that happens to fix the problem as a side effect. Scope creep in a fix is itself a risk: it expands the surface area that can introduce regressions, conflicts with the Feature Preservation Contract, and makes the change harder to review. If a larger structural improvement is also needed, it is a separate, explicitly lower-priority recommendation.

### Law 9 â€” Project Forward
Every audit finding exists in time. Before closing each section, ask:
> *"If this issue is not fixed, what does it cost in 6 months when the codebase is 2Ã— larger?"*
> A small validation gap that costs 5 minutes to fix today may require a data migration affecting thousands of records in 6 months. A naming inconsistency that is mildly confusing now becomes a maintenance trap when the team grows. An unabstracted localStorage call that takes 2 minutes to add now becomes a 2-week refactor before user accounts can be added.
> Time-amplified findings are marked with **â± COMPOUNDS** â€” these should be prioritized above their individual severity suggests, because the cost of inaction is not fixed.

### Law 10 â€” Knowledge Integrity
Domain data that cannot be read from the provided source code, has not been confirmed by the user in Â§0, and has not been found via a live web search with an official-quality source **must not be used as the basis for a finding**. This applies to:
- Game mechanics, live-service constants, stat values, formula coefficients
- API rate limits, pricing tiers, token limits, model context windows
- Medical reference values, drug dosing coefficients, clinical thresholds
- Financial rates, tax rules, regulatory thresholds
- Any numeric constant or rule specific to a third-party system, game, or platform

**Three valid paths to a correctness finding â€” in order of strength:**
1. User confirms the correct value in Â§0 â†’ `[Â§0-CONFIRMED]` â†’ finding can be filed
2. Official documentation or official patch notes found via web search, version-matched â†’ `[WEB: official, vX.Y, date]` â†’ finding can be filed with version caveat
3. Code value present, no external confirmation available â†’ flag as unverified, ask user

**Web search is not automatically reliable.** A web search that returns only community wikis, forum posts, or aggregators does not promote a fact from `[UNVERIFIED]` to confirmed. Only official sources do. Multiple conflicting web sources require the user to arbitrate â€” never the auditor.

**The correct behavior when domain data is needed but unavailable from code or Â§0:**
> "The code uses a value of `X` for `[constant]` [CODE: line N]. I searched for the official specification but found only `[source quality]`. Before filing a finding on this value, please confirm: what is the authoritative source, and what should `[constant]` be?"

**Never:** state a value from memory, then issue a finding based on whether the code matches it.
**Never:** use a community wiki or forum post as the sole basis for asserting "the code is wrong."
**Never:** silently pick one web source over another when sources conflict.
**Never:** silently "correct" a value recalled from a prior session that may itself have been wrong.
**Always:** source every domain fact explicitly â€” `[CODE]`, `[Â§0-CONFIRMED]`, `[WEB: source, version, date]`, or `[UNVERIFIED]`.

---

### Compound Finding Chains
Some bugs are individually minor but form chains of escalating harm when combined. Always look for:
- Validation gap [LOW] â†’ invalid value in engine [MEDIUM] â†’ wrong output displayed [HIGH] â†’ user makes bad real-world decision [CRITICAL given stakes]
- Missing cache invalidation [LOW] â†’ stale data served [MEDIUM] â†’ user acts on outdated info [HIGH] â†’ financial/health consequence [CRITICAL]

When a chain exists: document it, escalate the combined severity, and number the chain.

---

## III. EXECUTION PLAN

### Pre-Flight Checklist (Mandatory â€” Before Any Finding)

```
[ ] Read the entire source file(s) top-to-bottom without skipping
[ ] Classify: domain type, architecture pattern, app size â†’ determine part count
[ ] Extract all domain rules from code â†’ verify against Â§0 â†’ flag discrepancies
[ ] Identify all architectural constraints â†’ acknowledge them explicitly
[ ] Extract Design Identity from code if not provided â†’ confirm with user
[ ] Build Feature Preservation Ledger (every named feature: status + safety flags)
[ ] Map each critical workflow from Â§0 through the actual code
[ ] Identify top 5 risk areas based on domain classification
[ ] Announce: domain class, architecture class, planned part count, top-risk areas
[ ] For apps > 3,000 lines: wait for user acknowledgment before Part 2
```

### Part Structure

| Part | Focus | Non-Negotiable Deliverables |
|------|-------|-----------------------------|
| **P1** | Pre-Flight Â· Inventory Â· Architecture | Feature Preservation Ledger, Constraint Map, Design Identity Confirmation, Domain Rule Verification Table, Workflow Map, Audit Plan |
| **P2** | Domain Logic & Business Rules | Rule-by-Rule Verification, Formula Test Vectors, Data Accuracy Report, Temporal/Timezone Audit |
| **P3** | Security Â· Privacy Â· Compliance | Threat Model, Sensitive Data Inventory, Attack Surface Map, Compliance Gap List |
| **P4** | State Â· Data Integrity Â· Persistence | State Schema Audit, Validation Gap Report, Data Flow Diagram, Corruption Paths |
| **P5** | Performance Â· Memory Â· Loading | Web Vitals Estimate, Resource Budget Table, Memory Leak Inventory, Computation Bottlenecks |
| **P6** | Visual Design Â· Polish Â· Design System | Design Token Audit, Visual Rhythm Analysis, Component Quality Scorecard, Polish Gap Inventory |
| **P7** | UX Â· Information Architecture Â· Copy | Flow Analysis, IA Audit, Copy Quality Inventory, Interaction Pattern Audit |
| **P8** | Accessibility | Full WCAG 2.1 AA Checklist, Screen Reader Trace, Keyboard Nav Map, ARIA Correctness |
| **P9** | Browser Â· Platform Â· Compatibility | Cross-Browser Matrix, PWA Audit, Mobile/Touch Audit, Network Resilience Matrix |
| **P10** | Code Quality Â· Architecture Â· Optimization | Dead Code Inventory, Duplication Map, Naming Audit, Structural Analysis, Optimization Opportunities |
| **P11** | AI / LLM Integration *(activated when External APIs or AI/LLM field in Â§0 references any AI provider)* | Prompt Injection Surface, Output Sanitization Audit, Streaming Error Handling, Token/Cost Risk, Hallucination Exposure |
| **P12** | Internationalization & Localization | Hardcoded String Inventory, Locale-Sensitive Format Audit, RTL Audit, i18n Completeness Report |
| **P13** | Development Scenario Projection *(Â§O â€” see Growth Context in Â§0)* | Scale Cliff Analysis, Feature Addition Risk Map, Technical Debt Compounding Map, Dependency Decay Forecast, Constraint Evolution Analysis, Maintenance Trap Inventory |
| **P14+** | Domain Deep Dives | App-specific: probability math, financial precision, medical logic, AI integration, API contracts, etc. |
| **Final** | Summary Dashboard | Findings table, Root Cause Analysis, Compound Chains, Quick Wins, Optimization Roadmap, Polish Roadmap |

---

## IV. AUDIT DIMENSIONS

> 120+ dimensions across 15 categories. Every dimension applies to every app.
> Domain Classification (Â§I.1) determines depth and severity multipliers.

---

### CATEGORY A â€” Domain Logic & Correctness

The most consequential category. An app that looks polished but produces wrong output is harmful. Every point here verifies against Â§0 Domain Rules.

#### Â§A1. BUSINESS RULE & FORMULA CORRECTNESS
- **Constants verification**: Every named constant vs Â§0 expected value. Flag every discrepancy immediately, regardless of size.
- **Formula reproduction**: For every non-trivial formula â€” reproduce it by hand with known inputs and compare to actual code output.
- **Operator precision**: `>` vs `>=`, `&&` vs `||`, `Math.floor` vs `Math.round` vs `Math.ceil` â€” each one changes behavior at boundaries.
- **Order of operations**: Integer vs float division? Parenthesization correct? Associativity assumptions?
- **Precision strategy**: Does rounding happen at computation, at display, or both? Are rounding errors accumulating across a multi-step pipeline?
- **Units consistency**: Values that look similar but differ (percentages as 0â€“1 vs 0â€“100, ms vs s, cents vs dollars) â€” always handled correctly?
- **Domain invariants**: Properties that must always hold true (probabilities sum to 1.0, totals match line items, age â‰¥ 0) â€” are they enforced or just hoped for?
- **Boundary values**: Test the exact edges: `0`, `1`, `max_valid`, `max_valid + 1`, `-1`, `null`, `undefined`, `NaN`, empty string.

#### Â§A2. PROBABILITY & STATISTICAL CORRECTNESS *(deepened for gambling/gacha/actuarial/analytics)*
- **Model validity**: Is the mathematical model appropriate for the actual stochastic process (Markov, independence, memoryless)?
- **CDF integrity**: Does the cumulative distribution reach â‰¥0.9999 within the supported domain? Residual probability accounted for?
- **Expected value verification**: Computed EV matches closed-form solution where one exists?
- **Monte Carlo adequacy**: Sufficient trial count for the required precision? Standard error reported to user?
- **Numerical stability**: Underflow at very small probabilities? Overflow at very large inputs?
- **Known-good test vectors**: â‰¥3 manually-verified {input â†’ expected output} pairs tested against the engine.
- **Uncertainty communication**: Results labeled as estimates? Confidence intervals disclosed?

#### Â§A3. TEMPORAL & TIMEZONE CORRECTNESS
- **UTC offset correctness**: Every region's offset verified. Named timezones vs hardcoded offsets (named are safer).
- **DST transitions**: For any region with DST (US, EU, AU, etc.) â€” spring-forward and fall-back handled? Countdowns crossing a DST boundary?
- **Epoch arithmetic**: Timestamps in ms vs s â€” mixed? Far-future overflow?
- **Relative time**: "X days until" â€” off-by-one at midnight? Timezone-aware?
- **Scheduled events**: Daily/weekly resets â€” correct simultaneously for all supported timezones?
- **Stale temporal data**: Hardcoded dates that were correct at write-time but have since passed?

#### Â§A4. STATE MACHINE CORRECTNESS
- **Reachable invalid states**: Can the app arrive at a combination of state values that has no defined meaning?
- **Transition completeness**: Every event from every state â€” is the transition defined, or does some combination produce undefined behavior?
- **Guard conditions**: Transitions that should only fire under certain conditions â€” are guards actually enforced?
- **Race conditions**: Rapid clicks, concurrent tabs, Worker messages arriving simultaneously â€” state consistency maintained?
- **Idempotency**: Actions safe to repeat (refresh, double-click, re-import) â€” produce the same result?

#### Â§A5. EMBEDDED DATA ACCURACY
- **Named entity correctness**: Every named item (character, product, rate, rule, material) verified against authoritative source in Â§0.
- **Staleness**: Data accurate as of which version/date? Is that version documented? What has changed since?
- **Cross-reference integrity**: Entity A references Entity B â€” B exists and has the expected attributes?
- **Completeness**: All expected entities present? Gaps in coverage?
- **Relationship correctness**: Parent-child, lookup, many-to-many â€” all bidirectionally consistent?

#### Â§A6. ASYNC & CONCURRENCY BUG PATTERNS

These bugs are invisible in single-path testing but surface reliably in real usage.

- **Stale closure captures (React)**: `useEffect` callbacks capturing state/props via closure â€” if the dependency array is missing or incomplete, the effect runs with stale values. Classic symptom: a `setInterval` inside `useEffect` reads state that never updates after the initial render.
- **`async` in `forEach`**: `array.forEach(async item => { ... })` â€” `forEach` does not await Promises. All async operations fire simultaneously and errors are silently swallowed. Use `for...of` with `await` or `Promise.all()` instead.
- **Promise swallowing**: `.catch(() => {})` or bare `.catch(console.error)` with no recovery path â€” the operation silently fails, the app continues in a broken state the user cannot detect or recover from.
- **Unhandled Promise rejections**: `async` functions called without `await` and without `.catch()` â€” the rejection is unhandled. Modern browsers fire an `unhandledrejection` event but give the user no feedback. Search for every `asyncFn()` call pattern (no `await`, no `.then/.catch`).
- **Race condition on sequential async calls**: Two rapid user actions each fire an async request â€” the second resolves first, then the first overwrites the newer result. Fix: use `AbortController` to cancel the previous request, or track a request sequence number and discard stale responses.
- **Missing `useEffect` cleanup**: Effect creates a subscription, event listener, timer, or WebSocket â€” but the cleanup function (`return () => { ... }`) is absent. Causes resource leaks and React's "state update on unmounted component" warning.
- **Concurrent state writes from multiple effects**: Multiple `useEffect` hooks each calling `setState` on the same state slice, triggered by the same event â€” one effect silently overwrites another's result. Execution order is deterministic but non-obvious to future maintainers.
- **`setTimeout`/`setInterval` drift**: Using `setInterval` for a countdown timer â€” each tick drifts slightly due to JS event loop variance. After minutes, a visible desynchronization appears. Fix: use absolute timestamp deltas (`targetTime - Date.now()`).
- **Async constructor / mount pattern**: Critical initialization logic placed in `useEffect` â€” the component renders once with empty/default state before the effect runs. If no loading state is shown, the user sees a flash of wrong data or empty UI.

#### Â§A7. JAVASCRIPT TYPE COERCION & IMPLICIT CONVERSION TRAPS

JS silently converts types in ways that produce wrong results without throwing errors â€” the code runs, the numbers look plausible, and the bug is invisible until edge cases hit.

- **`==` vs `===`**: `"0" == false` â†’ `true`. `null == undefined` â†’ `true`. `[] == false` â†’ `true`. `"" == 0` â†’ `true`. Any `==` comparison with non-identical types is a potential silent misclassification. Search the entire codebase for `==` (not `===`) and assess each one.
- **`+` operator with mixed types**: `"5" + 3` â†’ `"53"`. Any `+` operation touching user input or API response data (which arrives as strings) silently concatenates instead of adding. Always explicitly convert: `Number(input) + 3` or `parseInt(input, 10) + 3`.
- **`parseInt` without radix**: Always `parseInt(str, 10)`. Also: `parseInt("3.5px")` â†’ `3` â€” stops at the first non-numeric character. Is that the intended behavior for inputs like "3.5rem", "10%", "N/A"?
- **`parseFloat` on formatted numbers**: `parseFloat("1,234.56")` â†’ `1`. Any user-formatted or locale-formatted number string must be normalized (strip commas, currency symbols) before parsing.
- **Falsy value cascade**: `0`, `""`, `null`, `undefined`, `NaN`, and `false` are all falsy. `if (count)` is `false` when `count === 0` â€” a common off-by-one source for zero-item states. `if (name)` is `false` when `name === ""`. Use explicit comparisons: `count !== null && count !== undefined` or `count != null` (intentional `!=`).
- **NaN propagation**: `NaN !== NaN` â€” the only value not equal to itself. `isNaN("hello")` returns `true`; so does `isNaN(undefined)` â€” they mean different things. Use `Number.isNaN()` for strict detection. Any arithmetic involving NaN silently produces NaN, which propagates through the entire calculation pipeline, ultimately displaying as `NaN` or `0` (after `|| 0` guards) with no error.
- **Array/object truth inconsistency**: `[]` and `{}` are truthy. `Boolean([])` â†’ `true`, but `[] == false` â†’ `true` via type coercion. Conditionals that expect to distinguish "no data" from "empty array" must use `.length` checks, not truthiness.
- **Numeric string comparisons**: `"10" > "9"` â†’ `false` (string comparison: `"1" < "9"`). If sort comparators or range checks operate on uncoerced string inputs, ordering silently fails for numbers â‰¥ 10.
- **`typeof null === "object"`**: Historical JS bug, unfixable. `if (typeof x === "object")` is true for `null`. Always add `&& x !== null` for any object type check.
- **Implicit global variable creation**: A variable assigned without `let`/`const`/`var` inside a function silently becomes a property on `window`. Is `"use strict"` enabled globally to catch this class at runtime?

---

### CATEGORY B â€” State Management & Data Integrity

#### Â§B1. STATE ARCHITECTURE
- **Schema completeness**: Every field â€” type, valid range, default value, null/undefined behavior, documented purpose.
- **Normalization**: Any piece of data represented in two places that can diverge? Single source of truth for everything?
- **Derived state staleness**: Computed values re-derived on demand vs cached â€” if cached, what invalidates the cache?
- **Initialization correctness**: Default state valid for both fresh install and state-restored-from-storage?
- **Reset completeness**: State reset/clear â€” leaves orphaned storage keys? Misses any field?

#### Â§B2. PERSISTENCE & STORAGE
- **Completeness**: Every user-meaningful state field persisted? Any transient UI state accidentally persisted?
- **Schema versioning**: Version identifier in stored data? Migration logic for schema evolution across app versions?
- **Quota management**: localStorage size monitored? User warned approaching 5MB? `QuotaExceededError` caught gracefully?
- **Concurrent write safety**: Multiple tabs writing simultaneously â€” race condition? Data loss?
- **Cold start validation**: Persisted state parsed and validated against current schema before use? Handles corrupted state from a previous bug?
- **Sensitive data in storage**: Tokens, passwords, PII stored unencrypted in localStorage?

#### Â§B3. INPUT VALIDATION & SANITIZATION
- **Coverage**: Every user-facing input validated â€” none bypassed?
- **Type enforcement**: Silent type coercion (`"0" == 0`, `parseInt(undefined)`) producing wrong values?
- **Range enforcement**: Min/max limits â€” enforced at input layer, computation layer, or display layer (or none)?
- **Boundary testing**: For each input: test `0`, `max`, `max+1`, `-1`, `""`, `null`, `NaN` â€” what happens?
- **NaN/Infinity propagation**: Division by zero? `parseInt("")` returning NaN silently becoming 0 in downstream math?
- **Validation UX**: Error messages tell the user what went wrong and what they should enter instead.

#### Â§B4. IMPORT & EXPORT INTEGRITY
- **Import safety**: `JSON.parse` in try/catch everywhere? Prototype pollution via `__proto__`/`constructor`/`prototype` keys?
- **Size enforcement**: Maximum import size enforced before parsing begins?
- **Schema validation**: Imported data validated against expected schema â€” not blindly spread into state?
- **Preview before commit**: User sees what will change before confirming?
- **Rollback capability**: Pre-import state snapshot saved? Import undoable?
- **Round-trip fidelity**: `export â†’ import â†’ export` â€” both exports identical?
- **Partial import**: Can user import a subset without overwriting unrelated state?
- **Export completeness**: 100% of user state in export? Anything missing?
- **Self-describing schema**: Version field and field descriptions in export JSON, so external tools can parse it?

#### Â§B5. DATA FLOW MAP
Produce a text diagram: `User Input â†’ Validation â†’ State â†’ Computation â†’ Display`
At every arrow: What can go wrong? What protection exists? What is the gap?

#### Â§B6. MUTATION & REFERENCE INTEGRITY

Mutation bugs are among the hardest to find â€” the code looks correct but silently operates on shared references, causing distant, non-reproducible state corruption.

- **Direct state mutation (React/Vue)**: `state.items.push(item)` or `state.count++` mutates the existing reference â€” the framework's reconciler sees the same reference and may not re-render, or renders with partially updated state. Always produce new references: `setState(prev => ({ ...prev, items: [...prev.items, item] }))`.
- **`Object.assign` shallow copy trap**: `Object.assign({}, state)` creates a shallow copy â€” nested objects and arrays are still shared references. Mutating a nested property mutates both the copy and the original. Use structured clone, spread recursively, or an immutability library for nested state.
- **`Array.sort()` and `Array.reverse()` mutate in place**: Calling `items.sort(compareFn)` in a render path or derived value mutates the source array. Use `[...items].sort(compareFn)` to sort a copy.
- **Shared default parameter objects**: `function createItem(options = DEFAULT_OPTIONS)` where `DEFAULT_OPTIONS` is a module-level object â€” if any caller mutates `options`, subsequent callers receive the already-mutated object as their "default". Always spread defaults: `{ ...DEFAULT_OPTIONS, ...options }`.
- **Closure accumulation across calls**: A function closes over an array or object and mutates it on every call â€” each invocation accumulates state from all previous calls, not starting fresh. Particularly subtle in callbacks registered during module initialization.
- **Props mutation (React)**: Directly mutating a prop value (e.g. `props.items.push(...)`) instead of triggering a parent state update â€” violates unidirectional data flow and causes stale state across renders in ways that are very difficult to trace.
- **Synthetic event object pooling (React < 17)**: Accessing `event.target.value` inside a `setTimeout` or async callback â€” React's synthetic event pool reuses the event object, so accessing it after the handler returns returns `null`. React 17+ removed pooling, but if React version is unknown: check all async event accesses.
- **Immer `produce` misuse**: Mutations outside the Immer draft context, returning both a mutation and a value from the same producer, or forgetting to return from a non-mutating producer â€” all cause silent state corruption that is extremely difficult to trace.

---

### CATEGORY C â€” Security & Trust

#### Â§C1. AUTHENTICATION & AUTHORIZATION
- **Credential storage**: Passwords, tokens, API keys â€” never in localStorage unencrypted, never in source code.
- **Hash comparison**: Client-side hash comparisons â€” constant-time? Hash visible in source (extractable for offline brute-force)?
- **Lockout bypass**: Attempt-rate limiting stored in localStorage â€” clearable by user to reset counter?
- **Session management**: Token expiry handled? Idle logout? Session fixation?
- **Privilege escalation**: Can a user manipulate localStorage/state to access features beyond their authorization?

#### Â§C2. INJECTION & XSS
- **innerHTML / dangerouslySetInnerHTML**: Any use? Is the content user-supplied or from an external source?
- **DOM-based XSS**: User strings inserted into `className`, `href`, `src`, `style`, `data-*` attributes?
- **Dynamic code execution**: `eval()`, `Function()`, `setTimeout(string)`?
- **URL injection**: User-controlled values concatenated into URLs? Open redirect?
- **CSS injection**: User values in inline `style` strings?

#### Â§C3. PROTOTYPE POLLUTION & IMPORT SAFETY
- **JSON.parse safety**: Every parse call in try/catch â€” including ones that "can't fail"?
- **Prototype pollution**: Imported objects merged/spread without filtering `__proto__`, `constructor`, `prototype`?
- **Property collision**: Imported data keys capable of shadowing expected application properties?

#### Â§C4. NETWORK & DEPENDENCIES
- **All HTTPS**: Mixed-content risk from any HTTP resource?
- **SRI (Subresource Integrity)**: `integrity` attributes on CDN `<script>` and `<link>` tags? Without SRI, a CDN compromise serves malicious code.
- **External data tracking**: Third-party image hosts, CDNs â€” user IP/referrer logged without disclosure?
- **CORS**: External API CORS handling correct? Credentials in cross-origin requests?
- **CSP**: Content Security Policy present? `unsafe-inline`/`unsafe-eval` requirements that undermine it?

#### Â§C5. PRIVACY & DATA MINIMIZATION
- **PII inventory**: What personal data is collected, stored, or transmitted? Is each piece necessary?
- **URL leakage**: State in hash/query params leaks via browser history, referrer headers, server logs?
- **Third-party fingerprinting**: CDNs, analytics, presence systems â€” disclosed to user?
- **Export sensitivity**: Export JSON contains data the user didn't know was being recorded?

#### Â§C6. COMPLIANCE & LEGAL
- **GDPR/CCPA**: Personal data processed? Right to deletion? Privacy policy linked?
- **Age restrictions**: Gambling-adjacent, adult, or violence content â€” age gating present?
- **IP/Copyright**: Third-party copyrighted assets used? Attribution and disclaimer present?
- **Financial regulations**: App gives financial advice? Regulatory disclaimer?
- **Medical regulations**: App gives health guidance? "Not medical advice" disclaimer prominent?
- **Accessibility law**: ADA/EN 301 549 obligations relevant to this app?

---

### CATEGORY D â€” Performance & Resources

#### Â§D1. RUNTIME PERFORMANCE
- **Main thread blocking**: Computations >50ms on the main thread â€” UI freeze during execution?
- **Worker offloading**: Expensive algorithms in a Worker? Message passing correct? Fallback if Worker unavailable?
- **Unnecessary re-renders** (React): Every component that re-renders when it shouldn't. `memo()` comparators correct? Missing `useCallback`/`useMemo` deps?
- **List virtualization**: Grids/lists with 100+ items â€” virtualization needed? Jank with current approach?
- **Layout thrashing**: Reading `offsetWidth`/`scrollHeight` inside a write loop? Forces repeated reflows.
- **Debounce/throttle**: High-frequency events (input, scroll, resize) handled without overwhelming the main thread?
- **Cold start computations**: Expensive work triggered on mount instead of lazily on demand?

#### Â§D2. WEB VITALS & LOADING
- **LCP**: Largest element on first load â€” blocked by scripts? Image without preload?
- **FID/INP**: Long tasks during load â€” time to interactive?
- **CLS**: Images without `width`/`height`? Dynamic content injected above existing content? Font reflow?
- **Render-blocking scripts**: CDN scripts without `defer`/`async` â€” which ones block first paint?
- **FOUC**: CSS loaded after content renders?
- **Parse time**: Large single-file apps â€” JS parse/compile on low-end mobile (4Ã— CPU throttle)?
- **Resource hints**: `preconnect`/`dns-prefetch` for CDN origins? `preload` for hero images?

#### Â§D3. RESOURCE BUDGET
Produce this table for the app:

| Resource | Source | Est. Size | Load Strategy | Critical Path? | Optimization? |
|----------|--------|-----------|--------------|----------------|--------------|
| App code | inline/CDN | ? kb | blocking | yes | ? |
| Framework | CDN | ~130kb gz | blocking | yes | lighter alt? |
| â€¦ | â€¦ | â€¦ | â€¦ | â€¦ | â€¦ |
| **Total** | | **? kb** | | | |

- 3G first-load estimate (total / ~1.5 Mbps)?
- What % of app code executes in a typical session (unused code ratio)?

#### Â§D4. MEMORY MANAGEMENT
- **Closure leaks**: Closures holding references to large objects that should be GC'd?
- **Event listener leaks**: Every `addEventListener` has a corresponding `removeEventListener` in cleanup?
- **Timer leaks**: Every `setInterval`/`setTimeout` cleared on unmount?
- **Worker lifecycle**: Terminated when no longer needed? Multiple instances accidentally spawned?
- **Blob URL revocation**: `URL.createObjectURL` â€” matching `URL.revokeObjectURL` called?
- **Computation array retention**: Heavy tables (DP, MC) released after use or held in closure?
- **Canvas/WebGL cleanup**: Contexts and canvases disposed on unmount?

---

### CATEGORY E â€” Visual Design Quality & Polish

> This category treats visual design as a professional discipline, not an afterthought.
> The goal is to make the app's existing design vision more **refined, consistent, and polished** â€” not to replace it with generic conventions.
> Â§0 Design Identity is protected throughout. All findings improve toward the app's own aesthetic, not away from it.

> **Deep visual work:** When this audit's Â§E findings reveal systemic visual design issues â€” or when the user specifically requests a design audit, asks to "make it feel like [X]", or references a named aesthetic â€” the `design-aesthetic-audit` skill should be invoked as a companion. It covers 95 sections of visual-design-specific analysis (component character, copy alignment, illustration, data viz, token architecture, state design, responsive character, source material intelligence) that go well beyond what Â§E covers here. Route to it via Â§COMPANION in that skill, which maps directly to Â§E/P6 in this audit.

#### Â§E1. DESIGN TOKEN SYSTEM
- **Spacing scale**: Is every padding and margin value from a coherent mathematical scale (4/8/12/16/24/32/48/64)? List every one-off value like `p-[13px]` or `margin: 7px`. Each one is a token debt.
- **Color palette architecture**: Is the color system built on a small set of intentional tokens, or are there dozens of slightly-different hardcoded values? List near-duplicate colors and consolidate candidates.
- **Typography scale**: List every unique `font-size` in the codebase. Do they form an intentional modular scale (e.g., 12/14/16/20/24/32px), or are there arbitrary in-between values?
- **Font weight semantics**: Is each weight (`normal`, `medium`, `semibold`, `bold`) used for a consistent semantic purpose? Mixing `font-bold` and `font-semibold` for "emphasis" is token inconsistency.
- **Border radius system**: Are `rounded-*` values consistent by component type? Cards all use the same radius? Buttons the same? Inconsistency in radius reads as unprofessional at a subconscious level.
- **Shadow hierarchy**: Is there a shadow scale (e.g., `sm` for cards, `md` for modals, `lg` for popovers)? Or arbitrary per-component shadows?
- **Z-index governance**: Is stacking order explicitly managed? List every z-index value used. Collisions between layers (modals, toasts, dropdowns, sticky headers)?
- **Animation token set**: Are duration values from a consistent set (e.g., 100/200/300/500ms)? Are easing curves consistent for the same type of motion?
- **Token naming as documentation**: Are token names semantic (what they *mean*) rather than presentational (what they *look like*)? `--color-action-primary` scales to theming and dark mode; `--color-blue-500` does not. A well-named token system is itself product documentation â€” and for paid/multi-tenant products, also scales to whitelabeling and multi-brand use. For any product nature, naming tokens semantically reduces the cost of every future visual change.

#### Â§E2. VISUAL RHYTHM & SPATIAL COMPOSITION
- **Vertical rhythm**: Is there consistent spacing between sections, between cards, between form groups? Inconsistent vertical spacing destroys the feeling of order even when individual components look fine.
- **Density consistency**: Similar components (cards, list items, table rows) have similar internal density. One card with 24px padding and another with 12px padding on the same screen reads as broken.
- **Alignment grid**: Do elements align to a consistent invisible grid? Are there elements that appear to "float" without visual anchoring?
- **Whitespace intention**: Is whitespace used actively to group related items and separate unrelated ones? Or is it applied without rhythm (some areas cramped, others sparse)?
- **Proportion**: Do related elements (label + value, icon + text, header + content) feel proportionally balanced?
- **Focal point clarity**: On every key screen â€” is there one clear visual focal point that draws the eye first? If the answer is "everything has equal visual weight," the design has no hierarchy and users don't know where to look. Identify the intended focal point on each primary view, then assess whether the current visual treatment actually draws the eye there.
- **Visual weight distribution**: Is visual mass (size, color saturation, contrast, bold weight) distributed intentionally across the screen? Heavy visual elements clustering in one corner makes the layout feel unbalanced. Scan each primary view for unintentional visual weight accumulation.

#### Â§E3. COLOR CRAFT & CONTRAST
- **Color harmony**: Does the accent color work harmoniously with the background and surface colors? Is there a clear hierarchy: background â†’ surface â†’ elevated surface â†’ accent?
- **Dark mode craft**: For dark themes â€” are dark surfaces using near-black with slight hue (e.g., `#0f1117` with a hint of blue) rather than pure black (except intentional OLED)? Pure neutral dark often reads as less refined than dark with character.
- **Accent consistency**: Is the accent color used consistently as an emphasis signal? Or does it appear so frequently that it loses meaning?
- **Color temperature coherence**: Does the palette stay within a consistent temperature range? A warm orange accent on a cool blue-gray dark surface creates subconscious tension unless intentional.
- **WCAG contrast compliance**: Every text/background combination meets 4.5:1 (normal text) or 3:1 (large/bold). Pay special attention to: muted grays on dark, colored text on colored backgrounds, placeholder text on inputs.
- **Non-text contrast**: UI components (input borders, icon buttons, focus rings) meet 3:1 (WCAG 1.4.11).
- **State colors**: Hover, active, disabled, error, success, warning â€” distinct, consistent, and on-brand?
- **Color psychology alignment**: Does the palette's psychological character match the app's emotional target (Â§0)? Blues and cool grays signal reliability and precision â€” appropriate for financial and medical tools. Warm oranges and greens signal energy and growth â€” appropriate for gamified or wellness tools. Misalignment between color psychology and domain creates subconscious friction.
- **Color saturation calibration**: Oversaturated colors (`#FF0000`, `#00FF00`) signal low craft regardless of product nature â€” a pure green is less refined than a calibrated `#14b8a6`. Assess the saturation and lightness of the palette: does it feel purposeful, or do any values feel like the first pick from a color wheel? *For paid/professional tools*: this directly affects trust and willingness to pay. *For fan/creative tools*: this affects whether the palette feels artistically considered or placeholder-level. The standard changes; the question doesn't.

#### Â§E4. TYPOGRAPHY CRAFT
- **Heading hierarchy**: Is there a clear visual hierarchy between h1/h2/h3/body/caption levels? Can a user scan the page and immediately identify the most important information?
- **Line length**: Body text lines ideally 45â€“75 characters. Very short or very long lines hurt readability.
- **Line height**: Body text typically 1.4â€“1.6Ã— for readability. Tight line height on dense text reads as cramped.
- **Font pairing**: If using multiple typefaces â€” do they complement or conflict? Consistent use of primary/secondary/monospace roles?
- **Letter spacing**: Display/heading text often benefits from slightly negative tracking (`-0.01em` to `-0.03em`) for refinement. Is this applied consistently to large text?
- **Text rendering**: `-webkit-font-smoothing: antialiased` applied for crispness on dark backgrounds?
- **Label quality**: Form labels, column headers, section titles â€” concise, sentence-case consistently applied, unambiguous?
- **Typography as character signal** `[A2][A3][A4]`: The typeface choice communicates personality before a single word is read. Assess whether the typeface matches the personality in Â§0, using the axis profile to determine what "correct" means for this specific app:
  - *High commercial intent (A1)*: Typeface credibility matters â€” a humanist sans (Inter, Plus Jakarta Sans) signals approachability; a geometric sans (DM Sans, Geist) signals precision; a transitional serif signals authority. Wrong tier here is a trust problem.
  - *Strong subject visual identity (A4)*: Does the typeface feel tonally coherent with the subject? A gritty crime drama tool using a soft rounded font, a classical music app using a harsh display face, a hiking companion using a cold corporate sans â€” all represent tonal mismatches between typeface and subject.
  - *Expert/practitioner audience (A3)*: Type density and precision are signals of domain competence. A clinical tool with oversized, rounded type feels like it's talking down to experts. A financial terminal with a decorative display font feels wrong.
  - *Emotionally sensitive / high-stakes context (A2)*: Typeface warmth, weight, and size directly affect emotional register. Sharp, compressed, or overly stylized typefaces increase anxiety in sensitive contexts.
  - *Aesthetic IS the product (A5)*: Typeface is part of the output's visual experience â€” the bar is highest here.
  Whatever the context â€” if the typeface contradicts the intended personality, name a specific alternative that would serve it better within the app's constraints.
- **Type craft signals** `[A1][A3][A5]`: The relevant refinements depend on the app's axis profile:
  - *High commercial / professional audiences*: tabular nums for aligned number columns (`font-variant-numeric: tabular-nums`), optical size adjustments for display text, consistent lining vs oldstyle figures, proper typographic quotes.
  - *Expert/dense-data contexts*: monospaced or tabular numerals for scannable data columns, appropriate weight for scanability under time pressure.
  - *Aesthetic-primary / creative tools*: OpenType features as expressive tools â€” ligatures, alternates, stylistic sets â€” used intentionally.
  - *Any product*: Is there any typographic personality (weight contrast, tracked caps, a purposeful accent) that makes the app feel designed rather than defaulted? Intentionality â€” not prestige â€” is the goal.

#### Â§E5. COMPONENT VISUAL QUALITY
- **Button states completeness**: Every button variant has all five states: default, hover, active/pressed, focus (keyboard-visible), disabled. Missing states feel broken during interaction.
- **Input field states**: Default, focus, filled, error, disabled. The focus ring must be clearly visible.
- **Card design quality**: Internal padding consistent. Border or shadow â€” not both unless intentional. Corner radius consistent. Content alignment consistent across all instances.
- **Badge/chip/tag design**: Consistent padding, radius, typography across all instances.
- **Modal/dialog quality**: Consistent backdrop opacity, border/shadow, corner radius, header/body/footer structure. Close button always in same position and same size.
- **Icon quality**: All icons from the same family at the same base size. Mixed icon families are visually noisy. Icons sized to optical weight, not just pixel dimensions.
- **Divider usage**: Lines/dividers used consistently â€” not as decoration but as structural separators. Too many dividers fragment the layout.
- **Image presentation**: Images consistently cropped (same aspect ratios for same context), with consistent corner radius treatment.

#### Â§E6. INTERACTION DESIGN QUALITY
- **Hover feedback**: Every interactive element has a perceptible hover state that communicates interactivity. Elements that look interactive but have no hover state confuse users.
- **Active/pressed feedback**: Pressing a button should feel physically responsive â€” typically a slight scale-down or color deepening.
- **Transition quality**: Transitions should feel deliberate and smooth. Abrupt appearance/disappearance, or overly long/bouncy transitions, break the professional feel.
- **Loading state quality**: Spinners vs skeleton screens â€” skeleton screens preserve layout and feel more polished for content-loading. Spinners are appropriate for actions.
- **Animation narrative**: Every motion should tell a story about the relationship between UI states. An element sliding in from the left implies it came from somewhere left. Fade-in from nothing implies it was created. Are animations telling the right story?
- **Empty state design**: Empty states are a design opportunity â€” they should be designed, not blank. Clear visual, helpful message, a clear call to action.
- **Error state design**: Inline errors positioned immediately adjacent to the field that caused them. Not just color â€” includes icon and descriptive text.
- **Animation as character signal** `[A2][A4][A5]`: The right motion vocabulary is derived from the axis profile â€” not from a product category:
  - *Focus-critical / high-stakes / high-frequency use (A2)*: Motion is a cognitive tax. Every animation must justify itself â€” does it serve the user's task, or serve visual interest? Lean toward 100â€“150ms, ease-out, nothing bouncy or attention-seeking.
  - *Emotionally sensitive contexts (A2)*: Abrupt or jarring transitions increase anxiety. Slow, smooth, and predictable motion is a safety requirement here, not a style preference.
  - *Creative / exploratory / leisure contexts (A2)*: Expressive motion is appropriate â€” spring physics, slight overshoot, personality without chaos.
  - *Strong subject visual identity (A4)*: The motion character can honor the subject's tonal register â€” urgency, calm, playfulness, weight â€” whatever the subject carries. This is a fidelity opportunity, not a decoration question.
  - *Aesthetic IS the product (A5)*: Animation may be the primary value â€” assess it as output quality, not UI polish.
  - *Any context*: Simple and consistent beats complex and inconsistent. One well-chosen transition applied throughout outperforms five different ones.
  Name the 1â€“2 specific timing or easing changes that would bring the motion vocabulary into alignment with this app's axis profile.
- **Delight moments** `[A1][A2][A4]`: The highest-impact moments for craft investment are derived from the use context and subject identity â€” not from a product tier:
  - *High-frequency tools (A2)*: The small moment that makes a daily tool feel good to use â€” a snappy response, a clean success state, an efficient transition at the right place.
  - *Emotionally sensitive tools (A2)*: Warmth and gentleness at key moments â€” a kind empty state, a calm confirmation, nothing abrupt when the user is vulnerable.
  - *Creative / expressive tools (A2)*: Moments that feel generative and alive â€” the tool responding as a collaborator, not just executing commands.
  - *Strong subject identity (A4)*: Moments that feel authentic to the subject and community â€” a result displayed in a way that resonates with how the audience experiences this subject.
  - *Any app*: The moment the app delivers its primary value â€” is it presented with any intentionality, or does the result just appear? Even a free utility benefits from treating its output moment with care.
  For each high-impact moment â€” is there any brief, purposeful visual acknowledgment? If not, it is a craft gap regardless of axes.
- **Physical responsiveness**: The best interfaces feel physical. Buttons compress, drawers slide, modals lift. Assess whether the interaction model feels flat and digital or has a quality of physical responsiveness â€” and whether that matches the product's intended personality.

#### Â§E7. OVERALL VISUAL PROFESSIONALISM
- **Design coherence**: Does the app feel like it was designed as a whole, or like different sections were designed independently? Incoherence is visible even when users can't articulate it.
- **Attention to detail**: Pixel-perfect alignment? No 1-pixel misalignments on borders? No slight gaps where elements should touch? Details matter at the professional level.
- **Brand consistency**: Is the app's visual identity consistent from section to section? Would a user recognize a new section as part of the same app?
- **Polish delta**: For each section â€” list specific changes that would move it from "functional" to "intentional" within the existing design language and axis profile.
- **Polish level assessment** `[A1][A2][A5]`: The right polish standard depends on the axes â€” not on a tier label:
  - *High commercial intent (A1)*: Verify these credibility signals â€” consistent 4/8-based spacing â€” subtle shadows with intentional offset and blur â€” smooth 200â€“300ms transitions â€” letter-spacing on headings â€” antialiased type â€” hover states that feel physical â€” skeleton loaders that mirror content shape â€” contextual empty states â€” confirmation animations on success.
  - *Focus-critical / invisible-aesthetic contexts (A2)*: The polish goal inverts â€” the absence of distraction IS the polish. Assess how little the interface demands of the user's attention while still feeling finished and trustworthy.
  - *Emotionally sensitive contexts (A2)*: Polish means warmth and safety â€” gentle corners, calm palette, generous spacing, transitions that feel unhurried. Clinical sharpness is a polish failure here.
  - *Aesthetic-primary contexts (A5)*: Polish means the UI chrome recedes so the output shines. Evaluate how well the interface disappears in favor of what it produces.
  - *Any app*: The universal baseline â€” is there one detail that clearly took extra effort? Does the app look *intentional* rather than defaulted? Is spacing consistent enough that nothing feels accidental?

#### Â§E8. PRODUCT AESTHETICS â€” DERIVED FROM AXIS PROFILE

> **This section is driven entirely by the Five-Axis profile from Â§I.4.** There are no fixed branches for "paid" vs "free" â€” instead, each question activates based on which axes are present. Run every tagged item whose axis is active in the profile; skip or substantially reframe items whose axis is inactive.

---

**`[A1]` COMMERCIAL INTENT ACTIVE** *(revenue-generating or institutional)*:
- **The first-impression credibility test**: Before the user reads a single word â€” does the composition signal "trusted tool" or "rough prototype"? List the 3 visual elements most undermining this credibility and the specific change that would fix each.
- **Visual trust hierarchy**: Does the palette feel stable and intentional? Does the typography feel appropriate for the domain? Does spacing feel designed or accidental? Trust is communicated visually before it is read.
- **Competitive visual benchmark**: Name the 2â€“3 most credible tools in this category. Compared to them â€” what does this app do better, at parity, or worse, in craft specifics?
- **Conversion or commitment blockers**: In any paid, sign-up, or institutional commitment flow â€” identify visual elements that undermine the user's confidence: unclear primary action, visual hierarchy that buries the CTA, absence of legitimacy signals.
- **Distribution channel fit** `[A1]`: What first-impression surface matters most for this app â€” App Store screenshot, sales demo, marketing page, Product Hunt listing? Is the visual design compelling in that specific context?

---

**`[A2]` USE CONTEXT: FOCUS-CRITICAL OR HIGH-STAKES**:
- **Cognitive load audit**: Identify every visual element that demands attention beyond what the user's task requires. Decorative elements, animations, color variety, complex backgrounds â€” each one is a cost. List everything that should be eliminated or minimized.
- **Information scannability**: Under time pressure or stress, can the user find the critical number, status, or action within 2 seconds? Is the most important information visually dominant?
- **Visual noise inventory**: List every element that could be removed, reduced, or quieted without losing functional information. In high-stakes contexts, visual noise is not a minor polish issue â€” it is a functional failure.

**`[A2]` USE CONTEXT: EMOTIONALLY SENSITIVE**:
- **Safety signals**: Does the visual design feel safe? Assess: corner radius (sharp corners feel clinical), color temperature (cold blues feel institutional), spacing (cramped layouts feel anxious), animation speed (fast transitions feel jarring). Identify the 2â€“3 specific changes that most increase felt safety.
- **Warmth calibration**: Is the palette warm enough for this emotional context without feeling saccharine? Is the typography gentle without being unreadable? Does the empty state feel welcoming or clinical?
- **Tone-design coherence**: Does the visual language match the emotional register the copy is attempting? A warm, reassuring message delivered inside a harsh, clinical layout creates dissonance.

**`[A2]` USE CONTEXT: CREATIVE OR EXPRESSIVE**:
- **Inspiration quality**: Does the interface itself feel inspiring, or purely functional? In a creative tool, the environment shapes the output â€” a beautiful, expressive interface puts users in a creative mindset.
- **Expressive range**: Is there room in the visual design for personality and surprise? Or is everything so controlled that the app feels sterile?
- **Chrome vs canvas**: How much visual space does the interface take from the user's creative work? Is the UI chrome earning its space?

**`[A2]` USE CONTEXT: LEISURE OR CASUAL**:
- **Delight calibration**: For a leisure tool, friction that would be acceptable in a professional context is not acceptable here. Is the experience genuinely pleasurable? Is there any moment of unexpected delight?
- **Low-stakes visual permission**: Leisure contexts allow more visual personality, playfulness, and even imperfection â€” provided it is intentional. Assess whether the current design uses this freedom, or applies professional-tool austerity where it isn't needed.

---

**`[A3]` AUDIENCE: EXPERT / PRACTITIONER**:
- **Density as respect**: Information density is a feature for expert users, not a flaw. Assess whether the current density level respects the expertise of the audience or talks down to them with excessive whitespace and simplified presentation.
- **Vocabulary accuracy**: Every label, stat name, unit, and domain term is a trust signal. One wrong term signals that the maker doesn't understand the domain. Audit every piece of domain vocabulary for precision.
- **Power-user surface area**: Are advanced capabilities accessible without being buried? Expert users should be able to do in 2 clicks what a novice does in 5 steps.

**`[A3]` AUDIENCE: MIXED OR BRIDGING**:
- **Progressive disclosure integrity**: The design must serve both expert and novice simultaneously. Is the complexity ladder clearly implemented â€” default view for novices, accessible depth for experts â€” without condescending to one or overwhelming the other?
- **Dual-register visual design**: Assess whether the visual design has a successful strategy for serving two different expertise levels. If it doesn't â€” this is a structural design problem that visual polish cannot fix.

---

**`[A4]` SUBJECT HAS STRONG VISUAL IDENTITY**:
- **Palette coherence**: Identify the dominant visual tones associated with the subject and assess whether the app's palette is *inspired by*, neutral to, or in conflict with them. Give a specific, actionable palette direction â€” not just "make it darker" but the specific character shift that would increase coherence.
- **Typographic tone**: Does the typeface feel tonally coherent with the subject? Identify a tonal mismatch if it exists and name a specific alternative.
- **Motion character**: Does the animation vocabulary honor the subject's energy, weight, and atmosphere? Name the specific adjustment that would increase alignment.
- **Iconography and visual register**: Do any custom icons or decorative elements feel consistent with the subject's visual language? Generic stock illustrations feel detached from a subject with a strong identity.

**`[A4]` COMMUNITY AESTHETIC NORMS EXIST**:
- **Insider signal audit**: What visual choices communicate that the maker is genuinely part of this community â€” familiar with its vocabulary, conventions, and tastes? What choices inadvertently signal an outsider? List both.
- **Anti-corporate check**: Does the visual design feel like it belongs to the community, or like it's trying to productize the community? Flag any design choices that feel like a startup trying to monetize a subculture â€” regardless of whether the product is actually paid.

**`[A4]` SUBJECT IS NEUTRAL / NO ESTABLISHED IDENTITY**:
- **Invented coherence**: With no subject identity to reference, the visual language must be invented entirely from within. Is there a coherent internal logic â€” a design concept or metaphor running through the product? If not, identify the strongest available candidate.

---

**`[A5]` AESTHETIC IS THE VALUE**:
- **Chrome restraint**: The UI interface around the product's output must recede as much as possible. Every pixel of interface competes with the product's own aesthetic output. Identify every non-essential UI element and recommend minimum-footprint alternatives.
- **Output quality assessment**: The visual quality of what the product *produces* â€” not just the container â€” must be assessed as a design output. Is it beautiful? Is it surprising? Does it feel like the tool is a creative collaborator?
- **Signature output quality**: Can a user immediately tell this output came from this tool? Is the output aesthetically distinctive?

**`[A5]` AESTHETIC MUST STAY INVISIBLE**:
- **Distraction inventory**: Every element that draws attention to itself is a failure. List every visual element that is "nice" but competes with the user's task â€” and recommend eliminating or reducing each.
- **Trust-through-clarity**: In invisible-aesthetic contexts, trust comes entirely from clarity and reliability, not from polish. Is every element present because it is functionally necessary? Are there any decorative elements that should be removed entirely?

---

**UNIVERSAL** *(always apply)*:
- **The "made with intent" test**: Does the app look like every visual decision was made deliberately â€” or like some things were shipped at their default? Identify the 3 visual elements most clearly signaling unintentional defaulting, and the specific changes that would make them look chosen.
- **App icon / favicon quality**: Legible at 16Ã—16 and all required sizes? Visually coherent with the app's design language? Distinct enough to be identified in a browser tab or home screen?
- **Visual coherence across sections**: Does the app feel designed as a whole? Would a user recognize a new screen as part of the same product?

#### Â§E9. VISUAL IDENTITY & RECOGNIZABILITY

> Identity means different things depending on the axis profile. For commercial products it is competitive differentiation. For community products it is subject fidelity. For creative products it is the distinctiveness of the output itself. Apply the questions whose axis is active.

- **Visual signature** `[A1][A4][A5]`: Can a user identify this app from a partial screenshot â€” a fragment of color, a component shape, a motion pattern, or the visual character of its output? Identify what could become a distinctive visual signature, or what already is one.
  - *Commercial (A1)*: Is the signature distinctive within the product category, or generic among competitors?
  - *Subject identity (A4)*: Does the signature feel like it belongs to the subject â€” or does it feel imported from a different visual world?
  - *Aesthetic-primary (A5)*: Is the output itself visually distinctive? Could the user recognize output from this tool versus a competing one?
- **Visual metaphor coherence** *(all)*: Is there a consistent design concept or visual logic running through the product â€” a coherent internal language? If one exists, is it consistent throughout? If none exists, what is the strongest candidate based on the subject, audience, and use context?
- **Accent color intentionality** `[A1][A4]`: Is the accent color purposeful â€” a calibrated hue with intentional saturation, not the first pick from a wheel? *Commercial*: Is it distinctive within the competitive landscape? *Subject identity*: Does it feel tonally connected to the subject?
- **Emotional arc design** *(all)*: Does the visual language guide users through the right emotional journey for this specific app and audience? Map the intended emotional arc (e.g., focus â†’ confidence â†’ satisfaction for a work tool; curiosity â†’ discovery â†’ delight for an exploratory tool; calm â†’ trust â†’ relief for a sensitive-context tool), then assess whether the visual transitions, state changes, and feedback moments support it.
- **Anti-genericness audit** *(all)*: Identify visual elements that make the app look interchangeable with a dozen others â€” same default palette, same component style, same layout conventions with no adaptation to the subject or audience. For each: what is the specific, minimal change that would make this element more distinctly *this* app?

#### Â§E10. DATA STORYTELLING & VISUAL COMMUNICATION

> Numbers and data are not just displayed â€” they are communicated. This section evaluates whether the app's visual language helps users understand, not just see.

- **Numbers as visual elements**: Are the most important metrics in the app displayed with visual weight proportional to their importance? A key output number in the same size and weight as a label fails visual information design. Identify every key number and whether its typographic treatment matches its significance.
- **Hierarchy of insight**: For data-forward apps â€” is there a visual path from "raw input" â†’ "computed result" â†’ "actionable insight"? Or does the user have to parse a flat grid of equal-weight numbers to find the answer to their question?
- **Chart design quality**: Every chart should answer a specific question. For each chart: state the question it is designed to answer â€” then assess whether the visual encoding (chart type, scale, color, label placement) answers that question as directly as possible. Common failures: pie charts for comparing more than 4 values, line charts for categorical data, bar charts where a table would communicate more precisely.
- **Progressive complexity revelation**: Does the design guide users from simple overview â†’ detailed drill-down â†’ power-user controls? Or does it present full complexity immediately? The visual design should embody progressive disclosure â€” not just the UX architecture.
- **Data density calibration**: Assess whether the information density is calibrated for the target audience. A tool for analysts can be dense; a tool for casual users must be generous with whitespace and explanation. Is the current density right? What is the cost to the app's usability of the current density choice?
- **Empty â†’ populated visual storytelling**: The transition from empty state to populated state is one of the most important visual moments in the product. Does populating data feel like the app coming alive, or does it feel like a spreadsheet being filled in? Identify the specific visual improvements â€” animation, color, layout shift â€” that would make this transition feel more meaningful.
- **Error as communication**: Error states should communicate clearly, not just signal failure. Does the visual design of error states match their urgency? A critical error and a mild warning should look visually distinct. Are error states designed with the same craft as the default states?

---

### CATEGORY F â€” UX, Information Architecture & Copy

#### Â§F1. INFORMATION ARCHITECTURE
- **Navigation model**: Do tab/menu labels and icons match users' mental model of the content? Would a new user find what they're looking for?
- **Content hierarchy**: Most important information visually prominent? Clear visual path from "input" to "output" to "action"?
- **Progressive disclosure**: Advanced/infrequently-used options hidden behind expandable sections? Or are all options shown at once overwhelming the user?
- **Categorization logic**: Is content grouped in ways that feel natural to the target audience? Groups should reflect user mental models, not implementation structure.
- **Section depth**: Is the navigation hierarchy the right depth â€” not so flat that everything is at the same level, not so deep that users lose track of where they are?

#### Â§F2. USER FLOW QUALITY
- **Friction audit**: For each workflow in Â§0 â€” count the steps. Are any steps unnecessary, confusable, or surprising? Every unnecessary step is a design failure.
- **Default value quality**: Are default values the most common/sensible choice? Good defaults dramatically reduce user effort.
- **Action reversibility**: Can users undo or go back from every action? Irreversible actions are acceptable if the user is clearly warned with enough context to make an informed decision.
- **Confirmation dialog quality**: Destructive confirmations tell the user specifically what will be destroyed and whether it is recoverable â€” not just "Are you sure?".
- **Feedback immediacy**: Does every action produce immediate visual feedback? Clicks that feel unresponsive damage trust.
- **Perceived performance**: During recomputation â€” does the UI show stale data, blank space, or a skeleton? Which is chosen, and is it the right choice?
- **Keyboard shortcuts**: For power users â€” are common actions keyboard-accessible? Are shortcuts discoverable (tooltip mentions it)?

#### Â§F3. ONBOARDING & FIRST USE
- **First impression**: On the very first visit, does the user understand what the app does and what to do first? Without tooltips or documentation?
- **Onboarding quality**: Does the onboarding teach by doing (interactive) or just describe (passive)? Interactive is more effective.
- **Onboarding re-entry**: Can users replay the onboarding? Can they access help at any time?
- **Empty state â†’ filled state**: The transition from "no data" to "data present" â€” is it visually satisfying? Does it feel like the app is gaining value?
- **Progressive complexity**: Does the app reveal complexity incrementally, or does it present everything at once?
- **Activation path clarity** `[A1][A2][A3]`: Is the visual hierarchy guiding the user toward their first meaningful interaction? What "meaningful" means depends on the axes:
  - *High commercial intent (A1)*: The path to first value must be visually direct â€” identify any elements that distract from or delay the activation moment.
  - *Expert audience (A3)*: Experts should reach their first productive action faster than novices, not be forced through the same beginner scaffolding.
  - *Casual / emotionally sensitive audiences (A3/A2)*: Is the function obvious without reading anything? Is the first step gentle enough not to intimidate?
- **First success moment design** `[A2][A4]`: The moment the user first achieves something meaningful is the highest-value moment in the product. Is it visually acknowledged? The right acknowledgment depends on context:
  - *Focus-critical tools*: A quiet, efficient confirmation â€” not celebration, just closure.
  - *Creative / leisure tools*: A moment of genuine visual satisfaction â€” the result feels like an output worth having.
  - *Community / subject tools*: The result presented in a way that resonates with how the community experiences the subject â€” using the right vocabulary, the right visual weight.
  - *Emotional / sensitive tools*: A warm, gentle affirmation â€” not enthusiasm, just reassurance.
- **Time-to-function legibility** *(all)*: Can a new user tell within 10 seconds what they will be able to do? This is a visual clarity question â€” the app's core function should be visually legible, not just textually stated.

#### Â§F4. COPY QUALITY
- **Tone consistency**: Does every piece of UI copy feel like it came from the same voice? List any copy that sounds notably different from the rest.
- **Clarity**: Every label, tooltip, placeholder, error message, and heading â€” is it unambiguous? Could a user unfamiliar with the domain understand it?
- **Conciseness**: UI copy should be as short as possible while remaining clear. List every piece of copy that could be tightened.
- **Terminology consistency**: The same concept always called the same thing. List every synonym pair or inconsistency.
- **Capitalization convention**: Title Case for navigation and headings, Sentence case for body text and labels â€” applied consistently?
- **Action verb quality**: Buttons should use strong, specific verbs: "Save draft" not "Submit", "Delete account" not "Confirm", "Import history" not "OK".
- **Empty state copy**: Empty states have a clear, helpful, action-oriented message â€” not blank or just "No data found."
- **Error message copy**: Human-readable, no jargon, explains the cause, explains what to do next.
- **Copy as commitment asset** `[A1]`: *Activate only for revenue-generating or institutional products.* In any paid, sign-up, or commitment flow â€” copy is conversion infrastructure. Does the CTA communicate value ("Start building") or just request action ("Sign up")? Does the copy build confidence or just inform? For each commitment-adjacent CTA, suggest a more compelling alternative.
- **Copy as domain fluency signal** `[A3][A4]`: *Activate when the audience has domain expertise or the subject has community vocabulary.* Does the copy use the community's or domain's natural vocabulary accurately â€” the terms, shorthand, and framing that practitioners and enthusiasts actually use? Copy that describes the subject the way a press release would, or uses domain terms loosely, signals distance from the audience. List any copy that feels written by an outsider and suggest alternatives that feel more native.
- **Copy as emotional register** `[A2]`: *Activate for emotionally sensitive, creative, or high-stakes contexts.* Does the copy's tone match the emotional context of use? Clinical language in a wellness tool, playful language in a high-stakes professional tool, bureaucratic language in a creative tool â€” all represent tone-design mismatches. Identify any copy that is tonally wrong for the use context.
- **Brand voice extraction** *(all)*: Based on the copy that exists, extract a 3-adjective voice descriptor. Then identify every piece of copy that violates this voice â€” too formal, too casual, too generic, or out of register for this app's axis profile.

#### Â§F5. MICRO-INTERACTION QUALITY
- **Hover states communicate intent**: Every interactive element has a hover state that feels intentional (cursor change, color shift, underline, elevation change).
- **Loading states**: Async operations have immediate feedback â€” even a short 200ms delay without feedback feels broken.
- **Success confirmation**: Successful actions are confirmed visually â€” save, copy, export, submit all acknowledge completion.
- **Scroll behavior**: Scroll-to-content after navigation? Scroll position preserved on back navigation? Smooth scrolling where appropriate?
- **Focus indicator quality**: Visible and styled to match the app's design language â€” not just the browser default blue rectangle (unless the design is minimal).

#### Â§F6. ENGAGEMENT, DELIGHT & EMOTIONAL DESIGN

> The goal of this section is derived entirely from the axis profile. "Engagement" means radically different things depending on whether the app is a high-frequency work tool, an emotionally sensitive companion, a community gift, or a creative instrument. Apply the questions whose axis is active.

**UNIVERSAL** *(all apps)*:
- **Reward moments**: When the user achieves something meaningful â€” does the UI visually acknowledge it? Even a brief, quiet confirmation transforms a functional interaction into a satisfying one. List every "achievement moment" in the core workflow and assess whether it has any visual acknowledgment. The *form* of that acknowledgment should match the axis profile â€” not all success moments should be celebrations.
- **Personality moments**: Are there interactions that reveal the app's character â€” an empty state with genuine voice, a micro-animation that feels considered, a transition that feels right? These are what users remember and describe to others. Identify 2â€“3 places where a personality moment would feel authentic to this app's axis profile.
- **Notification quality**: Any notification, badge, or alert indicator â€” designed with the same craft as the rest of the product? Unstyled browser alerts break the contract regardless of product nature.

**`[A1]` COMMERCIAL INTENT**:
- **Progress and investment visibility**: Can users see how far they've come, how much they've built? Progress signals create retention pull. Does the app leverage this without resorting to manipulative patterns?
- **Shareable outcomes**: Would a user want to share something they produced or achieved in this app? What is the most naturally shareable moment, and is it visually compelling enough to share?

**`[A2]` EMOTIONAL SENSITIVITY**:
- **Emotional safety in transitions**: Every state change â€” loading, error, empty, success â€” should feel emotionally appropriate to someone in a vulnerable state. Is there any moment that feels jarring, cold, or clinical where warmth was needed?
- **Absence of pressure patterns**: Are there any visual elements that create urgency, scarcity, or anxiety â€” even unintentionally? Countdown timers, red badges, aggressive empty states â€” all create pressure that is inappropriate in emotionally sensitive contexts.
- **"Feels like support" quality**: Does the app feel like it is on the user's side? What specific visual or copy choices most contribute to â€” or detract from â€” this feeling?

**`[A2]` CREATIVE / EXPLORATORY CONTEXTS**:
- **Discovery encouragement**: Does the interface visually invite exploration â€” or does it present a flat list of functions? Are there visual cues that suggest "there is more here to discover"?
- **Creative momentum**: Does the visual design maintain creative flow â€” or does it interrupt it with friction, confirmations, or loading states that break the user's concentration?

**`[A3][A4]` COMMUNITY / SUBJECT CONTEXTS**:
- **Community shareable moments**: Is there a moment in the app compelling enough that a user would screenshot it and share it in their community's space â€” a forum, a Discord, a social feed? Identify that moment and assess its visual quality for sharing.
- **Authentic delight**: Are there details that reward genuine familiarity with the subject â€” a label using community shorthand, a display that reflects how insiders think about the subject, a detail in an empty state that speaks directly to this audience's experience? These signals are disproportionately valuable for establishing insider credibility.
- **Integrity over manipulation**: Retention mechanics (streaks, FOMO, aggressive notifications) are tonally wrong when the app is a community gift or free tool â€” and often wrong even for paid tools. Flag any patterns that prioritize the product's engagement metrics over the user's actual experience.

---

### CATEGORY G â€” Accessibility

#### Â§G1. WCAG 2.1 AA COMPLIANCE

**Perceivable:**
- **1.1.1** â€” Every meaningful image has descriptive `alt` text. Decorative images have `alt=""`.
- **1.3.1** â€” Semantic HTML: `<button>`, `<nav>`, `<main>`, `<header>`, `<h1>â€“<h6>`, `<label>`, `<table>` used correctly â€” not `<div>` for everything.
- **1.3.2** â€” DOM reading order matches visual order.
- **1.3.3** â€” No instruction relies solely on sensory characteristic ("click the red button").
- **1.4.1** â€” Color not the only signal â€” status/error/success also conveyed by icon or text.
- **1.4.3** â€” All text: 4.5:1 contrast (normal), 3:1 (large/bold â‰¥18px or â‰¥14px bold).
- **1.4.4** â€” Text readable at 200% zoom without horizontal scroll.
- **1.4.11** â€” UI components and focus rings: 3:1 against adjacent colors.
- **1.4.13** â€” Tooltips dismissible, persistent, hoverable.

**Operable:**
- **2.1.1** â€” Every interactive element keyboard-reachable and operable.
- **2.1.2** â€” No keyboard traps (except intentional modal focus trapping).
- **2.4.1** â€” Skip navigation link for keyboard users to skip repeated content.
- **2.4.3** â€” Logical focus order follows visual reading order.
- **2.4.7** â€” Visible focus indicator on every interactive element.
- **2.4.11** â€” Focused element not fully obscured by sticky headers/overlays (WCAG 2.2).
- **2.5.3** â€” Button visible label text is included in the accessible name.
- **2.5.5** â€” Touch targets â‰¥44Ã—44px CSS.

**Understandable:**
- **3.1.1** â€” `lang` attribute on `<html>`.
- **3.2.1** â€” No unexpected context change on focus.
- **3.3.1** â€” Input errors identified in text, not just color.
- **3.3.2** â€” Every input has an associated label or clear instruction.

**Robust:**
- **4.1.2** â€” Custom interactive components: correct ARIA roles, names, states.
- **4.1.3** â€” Dynamic status messages (toasts, counters, results) announced via `aria-live`.

#### Â§G2. SCREEN READER TRACE
- Simulate reading the primary user workflow in DOM order only (no visual reference).
- Modal open/close: focus moves to modal on open, returns to trigger on close?
- Dynamic updates (results, timers, validation): announced via appropriate `aria-live` polarity?
- Icon-only buttons: `aria-label` present?
- Custom tabs, dropdowns, sliders: correct ARIA patterns (`role="tab"`, `aria-selected`, etc.)?

#### Â§G3. KEYBOARD NAVIGATION
- Tab through the full app â€” every interactive element reachable in logical order?
- Custom components (date pickers, sliders, carousels) â€” arrow key navigation?
- Modal focus trapping â€” Tab cycles within modal, cannot escape to page behind?
- Escape key closes dialogs/modals/dropdowns?
- Visible focus style matches the app design (not just default browser ring)?

#### Â§G4. REDUCED MOTION
- `prefers-reduced-motion: reduce` honored for ALL animations:
  - CSS transitions and `@keyframes` animations
  - JavaScript `requestAnimationFrame` loops
  - Canvas animations (requires explicit JS media query check â€” often missed)
  - Video/GIF autoplay
- Reduced motion removes non-essential animation but preserves state communication.

---

### CATEGORY H â€” Browser Compatibility & Platform

#### Â§H1. CROSS-BROWSER MATRIX

Build this table for the specific APIs and features the app uses:

| Feature Used | Chrome | Safari/iOS | Firefox | Samsung | Edge | Fallback? |
|-------------|--------|------------|---------|---------|------|-----------|
| Blob Worker/SW | âœ“ | âœ— | âœ— | ? | âœ“ | Required |
| `crypto.randomUUID` | âœ“ | 15.4+ | 92+ | ? | âœ“ | Math.random fallback |
| `backdrop-filter` | âœ“ | âœ“`-webkit-` | 70+ | ? | âœ“ | Graceful skip |
| `navigator.vibrate` | âœ“ | âœ— | âœ“ | âœ“ | âœ“ | No-op |
| `CSS.supports()` | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | â€” |
| `optional chaining ?.` | âœ“ | 14+ | 74+ | ? | âœ“ | â€” |
| `gap` in flexbox | âœ“ | 14.1+ | 63+ | ? | âœ“ | â€” |
| *[App-specific APIs]* | | | | | | |

For every âœ— or uncertain cell: does the app crash or degrade gracefully?

#### Â§H2. PWA & SERVICE WORKER
- **Blob SW incompatibility**: Firefox and Safari reject Blob URL service workers â€” graceful fallback?
- **Cache strategy correctness**: Cache-first for static assets, network-first for data, stale-while-revalidate for semi-static?
- **Version cleanup**: Old caches purged on app update? User ever stuck on stale JS?
- **Update notification**: User notified when a new version is deployed? Can they act on it?
- **Offline completeness**: Core functionality works offline? Network-dependent features fail gracefully?
- **Manifest completeness**: Required icon sizes (192Ã—192, 512Ã—512, 180Ã—180 iOS), `display`, `theme_color`, `background_color`, `start_url`, `scope`.
- **Install prompt**: `beforeinstallprompt` handled? iOS Add to Home Screen flow (no event) documented?

#### Â§H3. MOBILE & TOUCH
- **iOS Safari quirks**: `position: fixed` + virtual keyboard? `100vh` including address bar (use `dvh`)?
- **Android**: Back gesture in PWA â€” navigates back or exits app?
- **Touch vs hover**: Hover-only interactions blocked by `@media (hover: hover)`?
- **Safe area insets**: `env(safe-area-inset-*)` respected in fixed/absolute elements on notched devices?
- **Pinch-to-zoom**: `user-scalable=no` present? (Accessibility violation â€” WCAG 1.4.4)
- **Swipe gestures**: Conflict with native scroll? Threshold too sensitive for intentional scroll?

#### Â§H4. NETWORK RESILIENCE
- **CDN failure**: React/framework CDN unavailable â€” blank page or meaningful error?
- **Error boundaries**: React Error Boundaries around CDN-dependent components?
- **Third-party image failure**: Image host down â€” placeholder shown? Layout preserved?
- **Reconnection**: Back online after offline â€” state sync correct? Presence reconnects?
- **Timeout handling**: Operations that can hang indefinitely â€” timeout and error gracefully?

---

### CATEGORY I â€” Code Quality & Architecture

#### Â§I1. DEAD CODE & WASTE
- **Unused functions**: Defined but never called?
- **Unused constants**: Defined but never referenced?
- **Unreachable branches**: `if (CONSTANT === false)`, conditions that can never be true given state machine?
- **Commented-out code**: Old implementation left as dead comments â€” delete or document why it's kept.
- **Unused CDN libraries**: Loaded but never used?
- **Development artifacts**: `console.log`, `debugger`, `TODO`, `FIXME`, `HACK` â€” inventory and prioritize.

#### Â§I2. NAMING QUALITY
- **Casing conventions**: `camelCase` (variables/functions), `PascalCase` (components/classes), `SCREAMING_SNAKE` (constants) â€” consistent?
- **Semantic accuracy**: Functions that do more than their name says? Names that imply something different from actual behavior?
- **Boolean naming**: `is`/`has`/`can`/`should` prefix for boolean variables and props?
- **Event handler naming**: `on{Event}` for callbacks, `handle{Event}` for internal handlers â€” consistent?
- **Magic numbers**: Every unexplained numeric literal that should be a named constant. List all.
- **Unclear abbreviations**: What is `wsv`? `ctr`? `tmp2`? Either expand or document.

#### Â§I3. ERROR HANDLING COVERAGE
For every `try/catch` and every async operation:
- **Caught**: Is the exception caught, or does it bubble up to crash the app?
- **Logged**: Is there a trace for debugging (even in development)?
- **Surfaced**: Does the user see a meaningful message, or does the error disappear silently?
- **Recovered**: Does the app return to a valid, operable state?
- **Error boundaries**: React Error Boundaries at the right granularity â€” not just one global boundary that blanks the whole app?

#### Â§I4. CODE DUPLICATION
- **Logic duplication**: Same calculation in multiple places â€” which copy gets the bug fix?
- **UI pattern duplication**: Same component structure copied 3+ times â€” should be parameterized.
- **Constant duplication**: Same value hardcoded in multiple places â€” one change misses the others.
- **Copy-paste divergence**: Duplicated code where one copy was updated and the other wasn't â€” this is where bugs hide.

#### Â§I5. COMPONENT & MODULE ARCHITECTURE
- **Single responsibility**: Each component does one clearly-defined thing.
- **God components**: Components >300 lines doing multiple unrelated things â€” natural split points?
- **Prop drilling**: Props passed through 4+ intermediate components â€” context or composition?
- **Reusability**: Near-duplicate components that could be unified with a well-designed prop API.
- **Dependency direction**: Lower-level components not importing from higher-level state/context.

#### Â§I6. DOCUMENTATION & MAINTAINABILITY
- **Algorithm comments**: Non-trivial algorithms (probability engines, optimizers, state machines) have comments explaining the math, assumptions, inputs, outputs, and edge cases.
- **Lying comments**: Comments that describe what the code *used to* do before a refactor.
- **Architecture decisions**: Key choices documented with rationale (why single-file? why no server? why this state model?).
- **Section organization**: For large files â€” section index? Navigable by grep?
- **Changelog**: Version history maintained?

---

### CATEGORY J â€” Data Presentation & Portability

#### Â§J1. NUMBER & DATA FORMATTING
- **Numeric display consistency**: Same number formatted the same way everywhere (1,234 vs 1234)?
- **Percentage precision**: Contextual â€” 2dp for small values (<10%), 1dp for medium, 0dp for 100%? Consistent?
- **Date/time formatting**: Single format across all views? ISO 8601 for data, human-readable for display?
- **Currency formatting**: Locale-correct? Correct decimal places for the currency?
- **Null/zero/empty representation**: Consistent â€” `0`, `â€”`, `N/A`, hidden â€” same treatment for same meaning throughout?
- **Unit labels**: "45 items" not just "45". No ambiguous bare numbers.
- **Significant figures**: Contextual precision â€” casual context shows `~2.4 hrs`, not `2.41739012...`.

#### Â§J2. DATA VISUALIZATION QUALITY
- **Data correctness**: Chart data points map to correct domain values? Off-by-one errors?
- **Axis honesty**: Y-axis starting at 0 (unless explicitly justified)? No misleading truncation?
- **Scale choice**: Logarithmic vs linear â€” appropriate for the data range and user question?
- **Small value visibility**: Values near zero visible at default scale, or crushed to invisibility?
- **Tooltip accuracy**: Tooltip values match underlying computed values (not re-approximated)?
- **Visual vs computed agreement**: For every displayed number â€” the value shown equals the value computed.
- **Responsive correctness**: Labels overlap at narrow widths? Chart reflows on resize?
- **Colorblind safety**: Colors distinguishable without hue? (Use shape, pattern, or label as secondary encoding)

#### Â§J3. ASSET MANAGEMENT
- **Third-party image hosts**: Reliability? Rate limiting? GDPR implications?
- **Format modernity**: WebP/AVIF vs legacy PNG/JPEG?
- **Lazy loading**: `loading="lazy"` for below-fold images?
- **Error handling**: `onError` fallback image? No broken-image glyphs in the UI?
- **Alt text quality**: Meaningful descriptions (not filenames, not "image").
- **PWA icons**: All required sizes (192, 512, 180 for iOS)?

#### Â§J4. REAL-TIME DATA FRESHNESS
- **Staleness indicators**: Data that changes frequently (prices, statuses, counts) â€” does the UI communicate age? "Last updated 3m ago" vs silently stale?
- **Poll / push strategy**: Polling interval appropriate for data volatility? WebSocket reconnect on disconnect?
- **Optimistic updates**: Local state updated immediately, then confirmed or rolled back on server response â€” rollback path implemented?
- **Cache invalidation**: When does a cached response get considered stale? Can the user force-refresh?
- **Timestamp handling**: Server timestamps compared to client clock â€” timezone mismatch? Clock skew?
- **Race condition on rapid refresh**: Two in-flight requests, older response arrives after newer â€” does old data overwrite new?
- **Loading vs stale distinction**: Is there a visual difference between "this data is loading" and "this data might be outdated"?

---

### CATEGORY K â€” Specialized Domain Depths

Activate at maximum depth based on Â§0 Stakes and Â§I.1 Domain Classification.

#### Â§K1. FINANCIAL PRECISION
- **Integer cents rule**: All monetary values stored as integer cents/pence? Never float. `0.1 + 0.2 â‰  0.3` in IEEE 754.
- **Rounding discipline**: Explicit rounding at defined points â€” not relying on floating-point truncation.
- **Tax application order**: Before or after discount? Correct for jurisdiction?
- **Rounding rule**: Banker's rounding (round-half-to-even) vs standard rounding â€” which is legally required?
- **Multi-currency**: FX rate freshness? Which rate used for conversion?
- **Atomicity**: Can a partial operation (interrupted payment, network failure mid-transaction) leave state inconsistent?
- **Audit trail**: Financial actions logged immutably?

#### Â§K2. MEDICAL / HEALTH PRECISION
- **Formula source**: Every clinical formula cited against a published medical reference.
- **Unit safety**: Imperial/metric mixing? `mg` vs `mcg` confusion? `kg` vs `lbs`?
- **Dangerous value flagging**: Clinically dangerous values flagged prominently, not just displayed.
- **Disclaimer visibility**: "Not medical advice" prominent and impossible to miss.
- **HIPAA/equivalent**: Health data stored locally or transmitted? Regulatory requirements?
- **Uncertainty communication**: Model limitations stated? Estimates vs exact values labeled?

#### Â§K3. PROBABILITY & GAMBLING-ADJACENT
- **Model appropriateness**: Mathematical model valid for the actual stochastic process?
- **Worst-case disclosure**: Expected value shown alongside worst-case. Not just the average.
- **Spending escalation UX**: Does the UI design (with or without intent) guide users toward spending more?
- **Age verification**: Gambling-adjacent mechanics â€” is age gating present or required?
- **Jurisdiction**: Gambling regulations vary by country â€” is this app subject to any?

#### Â§K4. REAL-TIME & COLLABORATIVE
- **Conflict resolution strategy**: Two users editing simultaneously â€” last-write-wins, merge, or lock?
- **Presence accuracy**: Online/offline status stale? Reconnect latency?
- **Message ordering**: Out-of-order messages handled correctly?
- **Optimistic update rollback**: If a server operation fails â€” does the UI correctly roll back?

#### Â§K5. AI / LLM INTEGRATION
*(Activate when External APIs or AI/LLM field in Â§0 references any AI provider)*
- **Prompt injection via user input**: User-controlled text concatenated into a prompt â€” can a user inject instructions that change model behavior? Sanitize or clearly separate user content from system instructions.
- **Output sanitization**: AI-generated text inserted into the DOM via `innerHTML` or `dangerouslySetInnerHTML`? AI output can contain adversarial HTML/JS. Always treat LLM output as untrusted user input â€” escape or sanitize before rendering.
- **Markdown rendering XSS**: AI output rendered via a markdown library â€” is the library configured to sanitize HTML? (e.g., `marked` with `sanitize: true`, or `DOMPurify` post-process)
- **Token cost runaway**: Is there a `max_tokens` limit on every request? Can a user trigger unbounded completion chains (recursive calls, loops, tool use without depth limit)?
- **API key exposure**: API key in frontend source code, localStorage, or URL params â†’ extractable by any user. Keys must go through a backend proxy.
- **Model fallback**: If the primary model is unavailable or returns an error â€” graceful fallback or error message? No silent empty state?
- **Latency handling**: LLM calls are slow (1â€“30s). Is there a visible streaming indicator or progress state? Can the user cancel? Does the UI remain interactive?
- **Hallucination disclosure**: App presents AI-generated content as fact? Caveat required.
- **PII in prompts**: Does the prompt include user PII (name, health data, financial data)? Data processor obligations under GDPR/CCPA?
- **Rate limiting / retry**: 429 responses from the API â€” exponential backoff with jitter? User-visible message vs silent hang?
- **Streaming edge cases**: Partial chunk handling â€” what happens if stream cuts mid-token? Partial JSON in structured outputs?

---

### CATEGORY L â€” Optimization, Standardization & Polish Roadmap

> This category does not find bugs. It identifies opportunities to improve the app beyond "working" to "exceptional" â€” without introducing bugs, without removing features, without denaturing the design identity.

#### Â§L1. CODE OPTIMIZATION OPPORTUNITIES
- **Algorithm efficiency**: Are there O(nÂ²) operations where O(n log n) or O(n) is achievable without architectural change?
- **Memoization gaps**: Expensive pure computations called repeatedly with the same inputs â€” should be memoized.
- **Redundant computation**: Multiple places computing the same derived value â€” unify to a single derivation.
- **Bundle size reduction**: Dead imports? Lighter library alternatives that fit within the architectural constraints?
- **CSS optimization**: Unused CSS classes? Specificity conflicts? Long selector chains?
- **Render optimization**: Components that render on every global state change despite depending on only a small slice of state?

#### Â§L2. CODE STANDARDIZATION
- **Consistent patterns**: For similar problems (data fetching, error handling, form validation) â€” is one pattern used throughout, or multiple ad-hoc approaches?
- **Utility consolidation**: Repeated utility functions (date formatting, number formatting, string manipulation) that should be in a shared module.
- **Constant registry**: All domain constants in one place? Or scattered throughout the file?
- **Component API consistency**: Similar components with inconsistent prop naming (`onClose` vs `handleClose` vs `dismiss`)? Standardize.
- **Import/dependency order**: Consistent grouping and ordering of imports/CDN dependencies?
- **Error handling pattern**: Consistent try/catch shape and error reporting throughout â€” not a different approach in every async call.

#### Â§L3. DESIGN SYSTEM STANDARDIZATION
> The goal: move from "many components that each look fine individually" to "one coherent design system."
- **Token consolidation plan**: For every one-off spacing/color/radius value found in Â§E1 â€” provide the standardized token it should use and what currently uses that token.
- **Component variant audit**: For every component type (button, card, badge, input, modal) â€” list all existing variants, identify variants that should be merged or unified, identify missing variants the app needs but lacks.
- **Pattern library gap**: For components used â‰¥3 times without a shared implementation â€” recommend extraction to a shared component.
- **Theme variable completeness**: Every value that changes with theme (light/dark/OLED/brand) should be a CSS variable or theme token, never hardcoded. List every hardcoded value that bypasses the theme system.
- **Design system as product asset**: A coherent design system enables faster iteration, safer changes, and visual coherence as the product grows â€” for any product nature. Assess: does the current system have enough structure to support adding 5 new components without breaking the visual language? If not â€” what minimal token/component foundations would make it robust?
- **Theming readiness** *(paid/multi-tenant products only)*: If the monetization tier or distribution model involves multiple brands, clients, or customization â€” are brand-identity values (primary color, radius personality, font) isolated in a small set of root tokens that could be swapped per tenant? Flag this only when relevant â€” this is not a goal for a single-user or community tool.

#### Â§L4. COPY & CONTENT STANDARDIZATION
- **Voice guide**: Describe the app's copy voice in 3 adjectives, then list any copy that violates this voice.
- **Terminology dictionary**: For every key concept in the app, the canonical name. List synonyms used inconsistently.
- **Capitalization audit**: List every label, button, and heading â€” flag inconsistent capitalization.
- **Punctuation consistency**: Trailing periods in labels? Em-dashes vs hyphens? Consistent quotation marks?
- **Number/unit style**: Spelled-out numbers vs digits ("three" vs "3")? Consistent in same context?
- **CTA optimization**: Are calls-to-action specific enough? "Get started" â†’ "Create invoice" â†’ "Create your first invoice" â€” each more specific and more effective.
- **Brand voice guide deliverable** *(all)*: Based on the copy audit, produce a minimal voice guide for this specific app â€” derived from its axis profile, not a generic template. The guide should include:
  ```
  Voice: [adjective 1 / adjective 2 / adjective 3]
  Derived from: [Axis 1: commercial/non-revenue] Ã— [Axis 2: use context] Ã— [Axis 3: audience]
  
  This app sounds like: "[example]"  not  "[anti-example]"
  This app sounds like: "[example]"  not  "[anti-example]"
  
  Always: [rule 1], [rule 2], [rule 3]
  Never:  [rule 1], [rule 2], [rule 3]
  
  [A3/A4 if active] Domain/community vocabulary:
    Use: [terms the audience actually uses]
    Avoid: [generic substitutes that signal distance]
  
  [A2 if emotional/sensitive context] Tone floor:
    Never use language that: [specific tone restrictions for this context]
  ```
- **Copy quality as context-appropriate signal** *(all)*: Generic, utilitarian copy signals low craft in any context â€” but what "low craft" means varies by axis. For commercial tools it signals low ambition; for community tools it signals unfamiliarity with the subject; for sensitive-context tools it signals emotional tone-deafness; for expert-audience tools it signals domain ignorance. Identify the highest-priority rewrites based on the most active axes in this app's profile.

#### Â§L5. INTERACTION & EXPERIENCE POLISH
- **Transition coherence**: Every transition tells the correct spatial/relational story. Elements that appear from nowhere should instead grow, slide, or fade from a logical direction.
- **Delight opportunities**: Are there interactions that are currently functional but could be made memorable without adding visual noise? (Examples: subtle success animations, satisfying completion states, smooth drag interactions)
- **State change communication**: When something important changes (new calculation result, data saved, error cleared) â€” is the change communicated as an event, not just a static update?
- **Scroll experience**: Is scroll behavior intentional? Smooth scroll where appropriate? Scroll position preserved and restored correctly?
- **Loading sequence**: For multi-stage loading â€” does the sequence feel progressive (each stage appears in order) or jarring (everything appears at once)?
- **The craft implementation checklist** `[A1][A2][A5]` â€” derived from the axis profile:
  - *High commercial intent (A1)*: `transform: scale(0.97)` on button press â€” `transition: all 0.2s ease-out` on interactive surfaces â€” skeleton loaders that mirror actual content layout â€” `font-variant-numeric: tabular-nums` on number columns â€” focus rings styled to match the design language â€” hover states with appropriate cursor changes â€” contextual empty states â€” integrated notification system â€” success confirmation that closes the interaction loop.
  - *Focus-critical contexts (A2)*: Every transition under 150ms â€” zero decorative animation â€” information-forward layout with no competing visual elements â€” instant feedback on every interaction â€” nothing moves that doesn't need to move.
  - *Emotionally sensitive contexts (A2)*: All transitions 200â€“400ms minimum â€” ease-in-out curves only â€” no abrupt appearance/disappearance â€” warm confirmation states â€” gentle empty states â€” no red for anything non-critical.
  - *Aesthetic-primary contexts (A5)*: UI chrome transitions under 100ms so attention stays on the output â€” output presentation given full visual investment â€” no interface element competes with what the tool produces.
  - *Any app â€” universal baseline*: Is there at least one detail that clearly took extra effort? Does the app look intentional rather than defaulted? Is spacing consistent enough that nothing feels accidental? Do transitions feel considered rather than left at browser defaults?
- **Motion budget**: Every animation in the app consumes attention. Total the number of simultaneous animated elements a user might see at once. More than 2â€“3 simultaneous animations competes for attention and degrades perceived quality. Identify any views where the motion budget is exceeded and recommend which animations to reduce or remove.

#### Â§L6. PERFORMANCE POLISH
- **Render jank identification**: Identify specific interactions where frame drops are likely and suggest targeted fixes within architecture constraints.
- **Perceived performance improvements**: Even without changing actual speed â€” optimistic UI, instant visual feedback, skeleton screens that match real content shape, progressive disclosure of complex results.
- **Startup sequence optimization**: What is the minimum viable first render? What can be deferred? Can the critical path be reduced without changing functionality?
- **Memory footprint reduction**: Identify data structures that could be more memory-efficient without changing behavior.

#### Â§L7. ACCESSIBILITY POLISH *(beyond compliance â€” toward excellence)*
- **Landmark structure**: Is the page structure clear to screen reader users navigating by landmark? `<main>`, `<nav>`, `<aside>`, `<header>`, `<footer>` used intentionally?
- **Heading hierarchy excellence**: Not just technically correct â€” does the heading structure help a screen reader user understand the page structure and navigate efficiently?
- **ARIA live region tuning**: Are `aria-live` regions set to the right politeness level (`polite` for informational, `assertive` only for genuinely urgent)?
- **Focus choreography**: For complex interactions (modals, multi-step flows, wizards) â€” does focus movement tell a coherent spatial story?
- **Color-independent comprehension**: Can every piece of meaning in the app be understood in grayscale?

---

### CATEGORY M â€” Deployment & Operations

#### Â§M1. VERSION & UPDATE MANAGEMENT
- **Version single source of truth**: App version in one place in the codebase?
- **Schema migration**: State schema changes across versions â€” migration from old to new handled?
- **Rollback strategy**: Bad deploy â€” how do users get back to a working state?
- **Cache busting**: Static assets get new URLs when content changes?

#### Â§M2. OBSERVABILITY
- **Error reporting**: Uncaught exceptions â€” sent to error monitoring? At minimum, logged to console in a structured way?
- **Debug mode**: Development-only logging gated behind a flag (not `console.log` left in production)?
- **State inspection**: Can a developer inspect current application state without browser devtools?
- **Admin action logging**: Privileged actions logged? Immutable audit trail?

#### Â§M3. FEATURE FLAGS & GRADUAL ROLLOUT
- **Flag inventory**: List every `if (FEATURE_FLAG)` or `if (process.env.FEATURE_X)` in the codebase. Are the flags documented?
- **Dead flags**: Flags that are always true or always false in production â€” dead code that should be cleaned up?
- **Flag coupling**: Feature flags that must be toggled together â€” is this documented? Toggleing one without the other creates a broken state?
- **Emergency kill switch**: For risky or AI-powered features â€” is there a runtime flag to disable without a deploy?
- **A/B test cleanup**: Concluded experiments with flag code still in place â€” when is it scheduled for cleanup?

---

### CATEGORY N â€” Internationalization & Localization

> Activate at full depth whenever Â§0 `Locale / i18n` is not "English only" or is omitted.
> Even English-only apps should pass the hardcoded-strings check â€” future i18n cost compounds with every unchecked string.

#### Â§N1. HARDCODED STRING INVENTORY
- **User-visible strings in source**: Every string rendered in the UI that is hardcoded in JS/JSX/HTML rather than in a locale resource â€” list all.
- **Pluralization logic**: `"1 item" / "2 items"` â€” handled with `Intl.PluralRules` or equivalent? Not `count === 1 ? "item" : "items"` (fails in many languages).
- **Concatenated UI strings**: `"You have " + count + " messages"` â€” word order varies by language; must use a template/message format, not concatenation.
- **Hardcoded error messages**: Error strings in catch blocks, validation messages, toast content â€” all extractable?
- **Screen reader only text**: `aria-label`, `alt`, `title` â€” hardcoded or localizable?

#### Â§N2. LOCALE-SENSITIVE FORMATTING
- **Number formatting**: Uses `Intl.NumberFormat` (or equivalent) for display? Decimal separator differs: `.` (EN) vs `,` (DE, FR). Thousands separator differs. Hardcoded `toFixed(2)` is not locale-safe for display.
- **Date/time formatting**: Uses `Intl.DateTimeFormat` (or equivalent) for display? Month/day order, 24h vs 12h, calendar system all vary by locale.
- **Currency display**: `$1,234.56` is US-only. `Intl.NumberFormat` with `style: 'currency'` handles locale-correct formatting.
- **Collation/sorting**: String `sort()` uses byte order â€” not correct for non-ASCII text. `Intl.Collator` for locale-aware alphabetical sort.
- **Relative time**: `Intl.RelativeTimeFormat` for "3 days ago" style strings?
- **List formatting**: `"A, B, and C"` (Oxford comma, EN) vs `"A, B et C"` (FR). `Intl.ListFormat` handles this.

#### Â§N3. RTL (Right-to-Left) LAYOUT
*(Activate only if Â§0 Locale includes Arabic, Hebrew, Persian, Urdu, or other RTL languages)*
- **`dir="rtl"` on `<html>`**: Set dynamically per locale?
- **CSS logical properties**: `margin-inline-start` / `padding-inline-end` instead of `margin-left` / `padding-right` â€” the latter don't flip in RTL.
- **Flexbox direction**: `flex-direction: row` items reverse in RTL â€” intentional?
- **Icon mirroring**: Directional icons (arrows, chevrons, progress indicators) â€” should they flip in RTL? (Checkmarks and warning icons should not.)
- **Text alignment**: `text-align: left` should be `text-align: start` for RTL safety.
- **Canvas/SVG**: Custom rendering code â€” does it have RTL awareness?
- **Third-party components**: Date pickers, dropdowns, data grids â€” do they respect `dir="rtl"`?

#### Â§N4. LOCALE LOADING & PERFORMANCE
- **Bundle size**: All locale data bundled upfront vs loaded on demand? Loading all locales adds significant weight.
- **Fallback chain**: Missing key in current locale â†’ falls back to default locale â†’ falls back to key name? No blank UI?
- **Locale detection**: Browser `navigator.language` used for detection? User override persisted to storage?
- **Dynamic locale switch**: App re-renders fully in new locale without page reload? State preserved across switch?

---

### CATEGORY O â€” Development Scenario Projection

> This category looks **forward**, not backward. Every other category diagnoses what is wrong today.
> This category answers: what will this codebase become under normal development pressure, growth, and time?
> The output is not a list of bugs â€” it is a map of the future the developer is currently building toward,
> with specific forks where a small choice now prevents an expensive problem later.

#### Â§O1. SCALE CLIFF ANALYSIS

For every data-intensive, storage-bound, or computation-bound operation, identify the data volume at which it transitions from "works fine" â†’ "noticeably slow" â†’ "crashes or becomes unusable". Express as concrete thresholds, not vague warnings.

For each identified cliff:
```
Operation:       {e.g. "Filtering items list", "localStorage write on save", "O(nÂ²) sort"}
Location:        {specific function / component}
Current safe range:  {works acceptably up to N items / N KB / N concurrent actions}
Warning zone:    {degrades noticeably between N and M â€” user perceives lag}
Cliff edge:      {fails, freezes, or loses data above M}
Trigger:         {the specific user action or growth event that crosses this threshold}
Current trajectory: {estimated time to reach cliff at normal usage pace}
Fix window:      {how long the developer has before this becomes urgent}
```

Common cliff locations to analyze:
- **localStorage quota** (5MB hard cap): current payload size Ã— growth rate per user action
- **O(nÂ²) operations**: any sort + filter combination, nested loops over the same list, or `find()` inside a `map()`
- **Unvirtualized DOM lists**: lists rendered without virtualization â€” beyond 200â€“500 items, scroll jank becomes severe; beyond 2,000, the browser may freeze on initial render
- **Bundle parse time on mobile**: single-file apps growing past 500KB uncompressed are measurably slow to parse on mid-range Android (simulate with 4Ã— CPU throttle)
- **Re-render cascade**: a global state change that re-renders the entire tree â€” harmless at small scale, increasingly expensive as component count grows
- **Regex performance on large inputs**: pathological backtracking on user-provided strings

#### Â§O2. FEATURE ADDITION RISK MAP

Based on Â§0 `Likeliest Next Features` and reasonable inference from the app's domain and trajectory, identify the top 5 features most likely to be added â€” then analyze exactly what in the current codebase will break, resist, or require expensive redesign when each is added.

For each anticipated feature:
```
Feature:               {name}
Probability:           HIGH / MEDIUM (based on domain norms, code signals, Â§0 roadmap)

Current code that conflicts or must change:
  - {specific function/pattern} at {location} â€” {why it conflicts with this feature}
  - {specific assumption} baked into {component} â€” {why it breaks under this feature}
  - {data structure choice} â€” {why it requires redesign for this feature}

Pre-adaptation cost (fix now, before feature exists):  Trivial / Small / Medium / Large
Post-addition cost (fix after feature is already built): {estimated 3â€“10Ã— higher â€” why}

Pre-adaptation recommendation:
  {The minimal abstraction, interface, or structural change that opens the door for this feature
   without breaking any current behavior. This is not the feature itself â€” it is the preparation.}
```

Example conflicts to look for:
- **User accounts**: state stored flat (no `userId` scope) â†’ all data must be re-keyed; no concept of "current user" in state schema â†’ every component that reads state must be updated
- **Undo/redo**: state mutations applied directly â†’ no command history; immutable state + command pattern required
- **Multi-device sync**: localStorage as sole persistence â†’ no sync surface; no conflict resolution strategy
- **Theming / white-label**: hardcoded brand colors throughout â†’ cannot swap theme without touching hundreds of values
- **Server-side rendering**: `window`/`document` accessed at module level â†’ crashes during SSR; `localStorage` calls not guarded â†’ crashes on server

#### Â§O3. TECHNICAL DEBT COMPOUNDING MAP

Not all technical debt is equal. Some debt is inert â€” it stays roughly the same cost to fix forever. **Compounding debt grows in cost with every new line of code built on top of it.** Identify which current issues are compounding â€” these must be prioritized above their individual severity suggests.

Compounding debt markers:
- **Foundation coupling**: Logic that other features are being built directly on top of, without an abstraction layer. Every new feature deepens the coupling, making the foundation progressively harder to change.
- **Terminology divergence**: The same concept named differently in different sections â€” as the codebase grows and more developers touch it, the confusion multiplies with every new file that references both names.
- **Schema without migration infrastructure**: A stored data schema with no version field and no migration logic â€” every schema change risks silently breaking all existing users' stored state. The cost to add migration infrastructure compounds with every release that ships without it.
- **Test debt on changing code**: Frequently-modified logic with no test coverage. Every untested change increases the probability of an undetected regression. This compounds â€” the longer it goes without tests, the more likely existing behavior is already wrong, and the harder it is to add tests without first understanding what "correct" means.
- **Copy-paste architecture that has already diverged**: Duplicated logic where the copies are now subtly different. Each new feature must be applied to every copy; each copy is an independent bug surface. The longer this persists, the more the copies diverge.
- **Magic constants without a registry**: Domain-critical numbers scattered through the code without centralization. Every new formula that uses one of these values may use a different hardcoded version â€” silent inconsistency that compounds with every new formula.

For each identified compounding debt item:
```
Debt:                     {description}
Location:                 {where it lives in the codebase}
Current cost to fix:      Trivial / Small / Medium / Large
Cost multiplier (6 months): {estimated â€” e.g. "3Ã— harder after user accounts are added"}
Compounding trigger:      {the specific event or feature that causes the cost to jump}
Pre-emption recommendation: {the specific, minimal change that breaks the compounding cycle}
â± COMPOUNDS
```

#### Â§O4. DEPENDENCY DECAY FORECAST

For every external dependency (CDN script, npm package, third-party API), assess its forward risk profile.

| Dependency | Version | Maintenance Status | Risk Level | Specific Risk | Recommended Action |
|-----------|---------|-------------------|-----------|--------------|-------------------|
| {name} | {ver} | Active / Slow / Abandoned / Security history | LOW / MED / HIGH | {specific concern} | {action} |

Risk factors to assess for each:
- **Abandonment indicators**: No releases in 18+ months; single maintainer with reduced activity; issue response time > weeks; no responses to CVEs
- **Breaking change trajectory**: Frequent major versions; poor deprecation communication; current version many majors behind latest
- **Security history**: Prior CVEs â€” how quickly were they patched? Are there open unpatched vulnerabilities?
- **CDN single-point-of-failure**: Loaded from CDN without `integrity` attribute and with no fallback â€” a CDN compromise or outage causes catastrophic failure. Single CDN dependency for the entire app framework is a HIGH risk for any app with uptime expectations.
- **API version sunset**: External API endpoints with announced deprecation dates; versioned endpoints where the used version is no longer current
- **Framework compatibility drift**: Library last tested with framework version N; app now runs N+2; breaking changes in between are silent

#### Â§O5. CONSTRAINT EVOLUTION ANALYSIS

Based on Â§0 `Planned Constraint Changes` and natural growth pressure, analyze the migration complexity when the app outgrows each current constraint. The goal is to identify pre-adaptations â€” small, low-cost changes that make the eventual migration from 2 weeks of work to 2 days.

For each constraint likely to evolve:
```
Current Constraint:     {e.g. "localStorage-only persistence"}
Evolution Trigger:      {the growth or feature requirement that forces this change}
Migration Complexity:   LOW / MEDIUM / HIGH / PROHIBITIVE (if attempted without pre-adaptation)

Migration obstacles (specific â€” function/pattern names):
  - {what in the current code assumes this constraint and must be refactored}
  - {what data transformation is required for existing users' stored data}

Pre-adaptation opportunity:
  {The abstraction, interface, or structural change that can be added now at low cost
   that converts the eventual migration from a rewrite into a substitution.
   Cost now: {Trivial/Small}. Avoided cost later: {Medium/Large}.}
```

Key constraint evolutions to analyze by architecture type:
- **localStorage â†’ backend API**: Are all storage read/writes behind a service/repository abstraction? Or called directly from components? Direct calls mean every component must be updated during migration.
- **Single-user â†’ multi-user**: Is data stored with user scope (`userId` prefix) or flat? Flat storage requires a data migration affecting every existing user.
- **CDN imports â†’ build pipeline**: Do imports use bare specifiers (`import React from 'react'`) compatible with bundlers? Any `eval()`, `new Function()`, or string-based dynamic imports that break tree-shaking?
- **Hardcoded locale â†’ multi-locale**: What is the cost of string extraction? Are date/number formats centralized or scattered?
- **Monolith â†’ modular**: Circular dependency chains? Implicit shared global state between would-be modules? Which features are genuinely isolated vs. deeply entangled?

#### Â§O6. MAINTENANCE TRAP INVENTORY

Identify every location in the codebase that is disproportionately risky to modify â€” where a developer making what appears to be a simple change is at high risk of introducing a non-obvious regression.

For each maintenance trap:
```
Trap name:          {short descriptive name}
Location:           {specific function, component, or section}
Why it's a trap:    {the specific coupling, hidden dependency, or non-obvious behavior
                     that makes this section dangerous to touch}
Symptom signature:  {the error or failure mode a developer would see after accidentally breaking it}
Safe modification protocol: {the specific step-by-step check a developer must do before
                              and after any change to this section}
Defusion recommendation:    {the refactor that eliminates the trap â€” labeled separately from bug fixes}
```

Common maintenance trap patterns to scan for:
- **Functions with hidden side effects**: appear to compute a value but secretly mutate shared state, write to storage, or trigger network calls as a side effect â€” callers assume they are pure
- **Order-dependent initialization**: works only if functions or modules are called in a specific sequence, but nothing in the code enforces or documents this order
- **Load-bearing "magic" values**: constants whose specific values are non-obvious but critical â€” changing them "slightly" breaks unrelated functionality
- **Deep prop chains**: a prop value that flows through 5+ component layers â€” renaming or reshaping it requires updating every intermediate component
- **CSS specificity landmines**: a rule that overrides another rule via specificity, not structure â€” changing either rule breaks the other without any connection being visible in the source
- **Global state assumed by multiple components**: state that two or more components both read and write, with no coordination mechanism â€” changes to the write pattern silently break the read pattern

#### Â§O7. BUS FACTOR & KNOWLEDGE CONCENTRATION

Identify code sections that are effectively a **black box** â€” where the implementation is only safely modifiable by whoever wrote it, or where the only documentation is "don't touch this."

For each high-risk knowledge concentration:
```
Location:           {specific function/section}
Knowledge gap:      {what a new developer cannot understand from reading the code alone}
Bus factor risk:    {what breaks or becomes unmaintainable if the author is unavailable}
Minimum documentation:  {the specific comment or documentation that would make this safe
                          for a developer unfamiliar with the section to modify}
```

---

**Â§O â€” Required Output: Scenario Projection Summary**

This table must appear at the end of the Projection Analysis part:

| Scenario | Likelihood | Time Horizon | Current Readiness | Pre-adaptation Cost | Without Pre-adaptation |
|----------|-----------|-------------|-------------------|--------------------|-----------------------|
| {e.g. "User reaches 500+ items"} | HIGH | 3 months | NOT READY â€” cliff at 200 items | Small | Large refactor under pressure |
| {e.g. "Adding user accounts"} | HIGH | 6 months | PARTIAL â€” no user scoping in schema | Medium | Data migration + full state redesign |
| {e.g. "Moving to a backend"} | MEDIUM | 12 months | NOT READY â€” no storage abstraction | Small | Every component must be updated |
| {e.g. "React CDN major version bump"} | MEDIUM | 18 months | READY â€” no deprecated API usage | Trivial | Small |
| {e.g. "Second developer on the team"} | HIGH | Now | NOT READY â€” 3 maintenance traps, 2 knowledge concentration zones | Small docs effort | High onboarding risk |

---

## V. FINDING FORMAT

Every finding follows this exact format. No exceptions. No vague findings.

```
[SEVERITY] [CONFIDENCE] â€” {Short descriptive title}

Category:         {Â§Letter.Number} â€” {Category and Dimension Name}
Location:         Line {N} in {function/component name} / "near `functionName()`" if line unknown
Domain Impact:    {Connects finding to the app's actual purpose and user stakes}

Description:
  {What is wrong or suboptimal. References the exact variable, function, value, or CSS class.
   Never generic. "The `handleImport` function" not "the import function".}

Evidence:
  {For [CONFIRMED]: direct code reference or execution trace.
   For [LIKELY]: strong circumstantial code evidence.
   For [THEORETICAL]: the reasoning chain explaining why this is a risk.}

User Impact:
  {Concrete scenario: what actually happens to a real user because of this issue.}

Recommendation:
  {Specific, actionable. Before/after code snippet where feasible.
   If multiple approaches exist, rank them by safety and effort.
   Tag recommendations that change behavior with âš  BEHAVIOR CHANGE.
   Tag recommendations that touch design identity with âœ¦ IDENTITY-SENSITIVE.
   Tag findings whose cost grows over time with â± COMPOUNDS.}

Effort:        Trivial (<5 min) | Small (<1 hr) | Medium (<1 day) | Large (>1 day)
Risk:          {What could break if this recommendation is applied incorrectly}
Cross-refs:    {Other finding IDs that should be fixed together or in sequence}
Automated Test: {What test type (unit/integration/E2E/snapshot) and what assertion would catch a regression of this finding â€” e.g., "Unit test: `expect(calculateTax(100, 'FR')).toBe(20)` â€” currently returns 21 due to wrong rate"}
```

### Severity Scale

| Level | Meaning | When to Apply |
|-------|---------|---------------|
| **CRITICAL** | App-breaking crash, data loss, security breach, wrong output causing real harm | XSS vector, import corrupts all state, wrong financial/medical calculation, prototype pollution |
| **HIGH** | Major feature broken, significant user harm, serious accessibility failure, important data wrong | Broken workflow, missing critical validation, modal not keyboard-accessible, wrong domain data |
| **MEDIUM** | Feature partially wrong, degraded UX, moderate accessibility issue, noticeable inconsistency | Missing loading state, color contrast below 4.5:1, inconsistent number formatting, NaN not handled |
| **LOW** | Minor inconsistency, cosmetic issue, non-critical opportunity | Spacing inconsistency, slightly wrong animation timing, minor dead code |
| **NIT** | Style, naming, purely aesthetic with no UX impact | Typo in comment, inconsistent quote style, unused `console.log` in dev path |

**Stakes multiplier**: For CRITICAL-stakes apps, apply one level upward to findings in Â§A, Â§B, Â§C, Â§K.

**Optimization/Polish findings**: Use severity to express the improvement value, not a defect level. A HIGH polish finding means a high-value design improvement opportunity.

### Confidence Scale

| Level | Meaning |
|-------|---------|
| **[CONFIRMED]** | Verified by reading code and tracing execution path |
| **[LIKELY]** | Strong code evidence; near-certain but not runtime-verified |
| **[THEORETICAL]** | Architectural risk that cannot be confirmed without runtime testing |

---

## VI. REQUIRED DELIVERABLES

### Tier 1 â€” Must Complete (Parts 1â€“4)

| Deliverable | Format | Contents |
|-------------|--------|---------|
| **Feature Preservation Ledger** | Table | Feature Â· Status (Working/Broken/Partial/Unknown) Â· Dependencies Â· Safe to Modify Â· Safe to Remove Â· Notes |
| **Design Identity Record** | Summary | Confirmed design character, protected signature elements, any ambiguities resolved with user |
| **Architecture Constraint Map** | Table | Constraint Â· Why it exists Â· What breaks if violated Â· How recommendations respect it |
| **Domain Rule Verification Table** | Table | Rule from Â§0 Â· Code value/implementation Â· Match (âœ“/âœ—/âš ) Â· Finding ID if mismatch |
| **Workflow Trace Report** | Per-workflow | Each step Â· Code location Â· Bugs found at step Â· Pass/Fail |
| **Data Integrity Report** | Table | Input Â· Validation gap Â· Invalid values possible Â· Downstream corruption |
| **Priority Action Items** | Two-tier | Tier A: Quick Wins (CRITICAL/HIGH + Trivial/Small) Â· Tier B: Strategic (remaining CRITICAL/HIGH by user impact) |
| **Scenario Projection Summary** | Table | Scenario Â· Likelihood Â· Time Horizon Â· Current Readiness Â· Pre-adaptation Cost Â· Cost Without Pre-adaptation |

### Tier 2 â€” Should Complete (Parts 5â€“10)

| Deliverable | Contents |
|-------------|---------|
| **Sensitive Data Inventory** | Every stored/transmitted datum: classification, protection, risk |
| **Data Flow Diagram** | `Input â†’ Validation â†’ State â†’ Computation â†’ Display` with gap annotations at every arrow |
| **Graceful Degradation Matrix** | Dependency Â· Failure mode Â· User impact Â· Current fallback Â· Quality (Good/Partial/None/Crash) |
| **Resource Budget Table** | Resource Â· Size Â· Load strategy Â· Critical path? Â· Optimization opportunity |
| **Web Vitals Estimate** | LCP, FID/INP, CLS â€” each with bottleneck and fix |
| **WCAG 2.1 AA Scorecard** | Criterion Â· Pass/Fail/N/A Â· Evidence Â· Fix |
| **Cross-Browser Matrix** | Feature Ã— Browser: Pass/Fail/Partial/Unknown |
| **Design Token Inventory** | Every unique spacing, color, radius, shadow, z-index, transition â€” with consolidation plan |
| **Component Quality Scorecard** | Every component type: variant completeness, state completeness, visual consistency grade |
| **Copy Quality Inventory** | Every piece of UI copy: voice consistency, clarity, conciseness, suggested rewrites |
| **i18n Readiness Report** | Hardcoded string count, locale-unsafe format calls, RTL gaps, estimated i18n migration effort |

### Tier 3 â€” Complete if Time Allows (Parts 11+)

| Deliverable | Contents | Applies To |
|-------------|---------|------------|
| **Optimization Roadmap** | Code efficiency, render performance, bundle size â€” ranked by effort vs impact | All |
| **Design System Standardization Plan** | Token consolidation, component unification, pattern library gaps | All |
| **Polish Delta Report** | Per section: specific changes that move from "functional" to "intentional/professional" (framing adapted to product nature) | All |
| **Brand Voice Guide** | Voice adjectives, always/never rules, copy rewrites â€” adapted to product nature (community authenticity vs conversion copy) | All |
| **Commercial Readiness Assessment** | First-impression audit, competitive benchmark, monetization-tier alignment gap | Paid/Freemium/B2B only |
| **Thematic Fidelity Assessment** | Source-material color/type/tone alignment, community authenticity audit, fan credibility signals | Fan/Community tools only |
| **Visual Identity Report** | Brand signature, color/type/motion alignment to personality, differentiation or fidelity opportunities | All (framing varies) |
| **Missing Tests Matrix** | Critical code paths â†’ test type (unit/integration/E2E) â†’ priority | All |
| **Architecture Evolution Roadmap** | (1) Safe incremental improvements Â· (2) Medium-term refactors Â· (3) Long-term goals | All |
| **Domain-Specific Deep Dive** | Per Â§K dimensions activated by domain classification | All |

---

## VII. SUMMARY DASHBOARD (Final Part)

### Findings Table

| Category | Total | CRIT | HIGH | MED | LOW | NIT | Quick Wins |
|----------|-------|------|------|-----|-----|-----|------------|
| A â€” Logic | | | | | | | |
| B â€” State | | | | | | | |
| C â€” Security | | | | | | | |
| D â€” Performance | | | | | | | |
| E â€” Visual Design | | | | | | | |
| E8 â€” Product Aesthetics | | | | | | | |
| E9 â€” Brand Identity | | | | | | | |
| E10 â€” Data Storytelling | | | | | | | |
| F â€” UX/Copy | | | | | | | |
| F6 â€” Engagement/Delight | | | | | | | |
| G â€” Accessibility | | | | | | | |
| H â€” Compatibility | | | | | | | |
| I â€” Code Quality | | | | | | | |
| J â€” Data/Viz | | | | | | | |
| K â€” Domain | | | | | | | |
| L â€” Optimization | | | | | | | |
| M â€” Ops | | | | | | | |
| N â€” i18n/L10n | | | | | | | |
| O â€” Projection | | | | | | | |
| **Total** | | | | | | | |

### Root Cause Analysis

```
RC-{N}: {Root Cause Name}
Findings affected: F-001, F-007, F-012 (list all)
Description: The upstream condition that, if fixed, resolves multiple downstream findings
Fix leverage: Fixing this one root cause replaces {N} individual fixes
```

### Compound Finding Chains

```
Chain-{N}: {Name}
Combined Severity: {Escalated beyond individual findings}
  Step 1: [F-003] [LOW]  â€” {description}
  Step 2: [F-011] [MED]  â€” {description}
  Step 3: [F-019] [HIGH] â€” {description}
  Combined: {User harm scenario} â†’ {Severity at stakes level}
```

### Positive Verifications

{N} critical paths confirmed working correctly:
- `{feature}` â€” verified via {method} â€” no issues found

### Top 10 Quick Wins

Highest (severity Ã— user impact) with lowest effort â€” fix these first:

| # | ID | Title | Severity | Effort | Impact |
|---|----|----|---------|--------|--------|
| 1 | | | | | |

### Remediation Roadmap

```
IMMEDIATE â€” before next release:
  [ ] F-{id} {title} â€” Effort: {X} â€” Risk: {Y}

SHORT-TERM â€” next sprint:
  [ ] F-{id} ...

POLISH SPRINT â€” standalone improvement sprint:
  [ ] Design token consolidation â€” Effort: Medium
  [ ] Copy standardization â€” Effort: Small
  [ ] Component variant completion â€” Effort: Medium

MEDIUM-TERM â€” next 1â€“3 months:
  [ ] ...

ARCHITECTURAL â€” 6+ months:
  [ ] ...
```

---

## VIII. CROSS-CUTTING CONCERN MAP

| Concern | Dimensions | What to Watch For |
|---------|------------|-------------------|
| Floating-point precision | Â§A1, Â§A2, Â§J1 | Calculation drift â†’ wrong display â†’ user decisions |
| Theme completeness | Â§E1, Â§E3, Â§L3 | Hardcoded color bypassing theme â†’ inconsistency + a11y failure |
| Worker reliability | Â§D1, Â§H2, Â§H4 | Blob Worker browser incompatibility â†’ missing fallback â†’ wrong results |
| Storage limits | Â§B2, Â§I1 | Quota exceeded â†’ silent data loss â†’ corrupted reload |
| Timezone/DST | Â§A3, Â§A5 | Wrong DST offset â†’ wrong dates/countdowns â†’ wrong user decisions |
| Import/export chain | Â§B4, Â§C3, Â§C5 | Malformed import â†’ prototype pollution â†’ state corruption |
| External dependency failure | Â§H2, Â§H4, Â§J3 | CDN/image host down â†’ crash vs graceful degrade |
| Input boundary cascade | Â§A1, Â§B3, Â§D1 | Out-of-range value â†’ engine crash or wrong silent result |
| Reduced motion gap | Â§E6, Â§G4 | CSS respects prefers-reduced-motion; canvas/JS often does not |
| Stale cache on deploy | Â§H2, Â§M1 | SW serves old JS with new schema â†’ silent corruption |
| Validation gap chain | Â§B3, Â§A1, Â§F4 | Missing validation â†’ wrong logic â†’ wrong display â†’ user harm |
| Semantic HTML gap | Â§G1, Â§G2, Â§G3 | `<div>` buttons â†’ no keyboard, no screen reader, no WCAG |
| Design token fragmentation | Â§E1, Â§L3 | One-off values throughout â†’ visual inconsistency + maintenance burden |
| Copy inconsistency | Â§F4, Â§L4 | Same concept named differently â†’ user confusion + unprofessional feel |
| Concurrent state modification | Â§A4, Â§B2 | Multiple tabs / rapid actions â†’ race condition â†’ data corruption |
| Locale assumption | Â§N1, Â§N2, Â§J1 | Hardcoded decimal/date formats â†’ wrong display in non-English locales â†’ user confusion or data entry errors |
| AI output injection | Â§K5, Â§C2 | LLM-generated content inserted via innerHTML without sanitization â†’ XSS from adversarial model output |
| Feature flag coupling | Â§M3, Â§A4 | Flags that must be toggled together â€” missing one creates a permanently broken state invisible to the developer |
| Stale closure cascade | Â§A6, Â§B1 | Missing useEffect deps â†’ effect reads stale state â†’ computation uses wrong value â†’ displayed result is wrong |
| Type coercion in validation | Â§A7, Â§B3 | User input arrives as string â†’ `+` concatenates instead of adds â†’ invalid value passes validation â†’ corrupts downstream computation |
| Mutation through abstraction layers | Â§B6, Â§B1 | Shallow copy of state passed to child â†’ child mutates nested object â†’ parent state silently modified â†’ inconsistent renders |
| Compounding constraint assumption | Â§O5, Â§B2 | Every new component reads localStorage directly â†’ migration to backend eventually requires touching every component |
| Scale cliff invisibility | Â§O1, Â§D1 | O(nÂ²) operation works fine at development data volume â†’ cliff is invisible until production load â†’ no warning before failure |
| Design-nature mismatch | Â§E8, Â§E9, Â§L3 | Visual polish misaligned with product nature â€” paid tools with hobby aesthetics block conversion; free tools with corporate polish feel inauthentic; fan tools with generic design feel detached from source material |
| Brand identity absence | Â§E9, Â§E7 | No distinctive visual signature â†’ app is indistinguishable from competitors â†’ users don't remember it, don't recommend it, don't feel attached |
| Copy-tier mismatch | Â§L4, Â§F4 | Generic utilitarian copy in a paid product â†’ signals low ambition â†’ undermines trust established by visual design |
| Delight debt | Â§F6, Â§L5 | No reward moments, no personality signals, no visual differentiation â†’ product feels transactional â†’ lower retention, lower word-of-mouth |
| Color psychology conflict | Â§E3, Â§E9 | Palette emotional register mismatched to domain â€” e.g. aggressive reds in a wellness app, or playful pastels in a financial tool â†’ subconscious friction undermines trust |
| Domain data fabrication | Â§A1, Â§K1â€“K5, Â§I.5 | Domain constant or rate recalled from training memory or sourced from a low-quality web source (forum, community wiki) rather than from code, Â§0, or official docs â†’ fabricated or unverified finding basis â†’ developer acts on false information |
| Cross-audit contradiction | Â§0 Cross-Audit, Â§I.5 | Second audit silently produces a different value for the same domain rule â†’ one or both values is wrong â†’ developer gets conflicting guidance with no signal that a conflict exists |

---

## IX. FINAL MANDATE

**This audit is complete only when every finding is specific enough that the developer can implement it without asking a single follow-up question.**

"Improve error handling" fails this test.
"`handleImport()` near line 847 calls `JSON.parse(pastedText)` without a try/catch block â€” any non-JSON clipboard content throws an uncaught TypeError that crashes the entire React tree, requiring a page reload. Wrap in try/catch, catch the error, and display a toast: 'Clipboard content is not valid JSON. Please check your data and try again.'" passes this test.

**Every domain fact in a finding must carry a source tag: `[CODE]`, `[Â§0-CONFIRMED]`, or `[UNVERIFIED]`.** A finding whose correctness depends on an `[UNVERIFIED]` value is not a finding â€” it is a question. Present it as such. The developer's trust in this audit is built finding by finding; a single fabricated constant that the developer can immediately disprove destroys the credibility of every other finding.

**When this is not the first audit of this app:** the Prior Audit Continuity block in Â§0 is mandatory. Prior `[Â§0-CONFIRMED]` rules carry forward. Any conflict between sessions is surfaced immediately as `[CONFLICT]` â€” not silently resolved in either direction. The developer must arbitrate conflicts; the audit must not.

**The audit serves the app's vision, not a generic idea of what an app should look like.** Every optimization, polish recommendation, and standardization suggestion must make the app more *itself* â€” not more generic.

**The audit respects product nature â€” with precision.** The Five-Axis framework (Â§I.4) exists because "paid vs free" and "professional vs fan" are not the right dimensions. A nurse's dosing calculator, a meditation app, a generative art toy, a local community board, and a Wuthering Waves companion all require entirely different aesthetic reasoning â€” even if some share the same revenue model. The correct question is never "does this look like something worth paying for?" as a universal standard. The correct question is always: what does this specific app need to look like, given who uses it, how they use it, why they care about it, and what the design is trying to accomplish within that context?

**The audit spans time, not just the present state.** Findings marked â± COMPOUNDS are not cosmetic â€” they are the highest-leverage items in the entire audit because their cost increases with every week of delay. The Scenario Projection Summary (Â§O) is a first-class deliverable, not an optional appendix.

**Do not attempt this audit in a single response.** Follow the Execution Plan (Â§III). Begin with Part 1. Announce total planned part count. Build the Feature Preservation Ledger and Design Identity Record. For apps exceeding 3,000 lines, confirm with the user after Part 1 before proceeding.

**Part 1 begins with:** Reading the entire codebase. Classifying domain, architecture, and size. Applying Adaptive Analysis Protocols (Â§I.7) â€” note quality variance, partial codebase gaps, and any mid-audit reclassification triggers. Extracting and verifying every domain rule. Confirming the Design Identity. Building the Feature Preservation Ledger. Announcing the audit plan. Then waiting.
